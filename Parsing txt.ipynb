{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you extracted following files into the same directory as this notebook:\n",
    "- training_50_dih.joblib\n",
    "- validation_dih.joblib\n",
    "- le.joblib\n",
    "- ohe.joblib\n",
    "\n",
    "And run 'pip install joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dihedral(p):\n",
    "    \"\"\"Praxeolitic formula\n",
    "    1 sqrt, 1 cross product\"\"\"\n",
    "    p0 = p[0]\n",
    "    p1 = p[1]\n",
    "    p2 = p[2]\n",
    "    p3 = p[3]\n",
    "\n",
    "    b0 = -1.0*(p1 - p0)\n",
    "    b1 = p2 - p1\n",
    "    b2 = p3 - p2\n",
    "\n",
    "    # normalize b1 so that it does not influence magnitude of vector\n",
    "    # rejections that come next\n",
    "    b1 /= np.linalg.norm(b1)\n",
    "\n",
    "    # vector rejections\n",
    "    # v = projection of b0 onto plane perpendicular to b1\n",
    "    #   = b0 minus component that aligns with b1\n",
    "    # w = projection of b2 onto plane perpendicular to b1\n",
    "    #   = b2 minus component that aligns with b1\n",
    "    v = b0 - np.dot(b0, b1)*b1\n",
    "    w = b2 - np.dot(b2, b1)*b1\n",
    "\n",
    "    # angle between v and w in a plane is the torsion angle\n",
    "    # v and w may not be normalized but that's fine since tan is y/x\n",
    "    x = np.dot(v, w)\n",
    "    y = np.dot(np.cross(b1, v), w)\n",
    "    return np.arctan2(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_order = [\"[ID]\", \"[PRIMARY]\", \"[EVOLUTIONARY]\", \"[TERTIARY]\", \"[MASK]\"]\n",
    "\n",
    "# change this path to match the path of where you have the\n",
    "# training_50 and validation files\n",
    "txt_data_path = '/home/mikey/Data/ProteinNet/casp7_txt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def count_protein(raw_txt_data):\n",
    "    data = filter_line_end(raw_txt_data)\n",
    "    result = 0\n",
    "    for line in data:\n",
    "        if line == data_order[0]: #id\n",
    "            result += 1\n",
    "    return result\n",
    "\n",
    "def filter_line_end(data):\n",
    "    return [str_.replace('\\n', '') for str_ in data]\n",
    "\n",
    "def get_primary_from_all_data(data, lim):\n",
    "    result = []\n",
    "    protein_count = 0\n",
    "    flag = False\n",
    "    for line in data:\n",
    "        if line == data_order[2]: #evolutionary\n",
    "            flag = False\n",
    "        if flag:\n",
    "            result.append(line)\n",
    "        if line == data_order[1]: #primary\n",
    "            flag = True\n",
    "        if line == data_order[0]: #id\n",
    "            protein_count += 1\n",
    "        if lim and protein_count > lim:\n",
    "            return result\n",
    "    return result\n",
    "\n",
    "def get_evolutionary_from_all_data(data, lim):\n",
    "    result = []\n",
    "    protein_count = 0\n",
    "    flag = False\n",
    "    for line in data:\n",
    "        if line == data_order[-2]: #mask\n",
    "            flag = False\n",
    "        if flag:\n",
    "            result.append(line)\n",
    "        if line == data_order[-3]: #tertiary\n",
    "            flag = True\n",
    "        if line == data_order[0]: #id\n",
    "            protein_count += 1\n",
    "        if lim and protein_count > lim:\n",
    "            return result\n",
    "    return result\n",
    "\n",
    "def get_tertiary_from_all_data(data, lim):\n",
    "    result = []\n",
    "    protein_count = 0\n",
    "    flag = False\n",
    "    for line in data:\n",
    "        if line == data_order[-1]: #mask\n",
    "            flag = False\n",
    "        if flag:\n",
    "            result.append(line)\n",
    "        if line == data_order[-2]: #tertiary\n",
    "            flag = True\n",
    "        if line == data_order[0]: #id\n",
    "            protein_count += 1\n",
    "        if lim and protein_count > lim:\n",
    "            return result\n",
    "    return result\n",
    "\n",
    "def group_aminoacids_together(data, every_n):\n",
    "    data_expanded = [np.expand_dims(t.split('\\t'), 1) for t in data]\n",
    "    result = []\n",
    "    for i in tqdm(range(0,len(data_expanded),every_n)):\n",
    "        # group together every_n entries (e.g. 3 for tertiary and 21 for evo)\n",
    "        result.append(np.concatenate([data_expanded[i+r] for r in range(every_n)], axis=1))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def parse_tertiary_from_file(path, data_lim=None):\n",
    "    with open(path) as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    data_ = filter_line_end(data[:data_lim])        \n",
    "    only_tertiary = get_tertiary_from_all_data(data_, data_lim)\n",
    "    return group_aminoacids_together(only_tertiary, every_n=3)\n",
    "\n",
    "def parse_evolutionary_from_file(path, data_lim=None):\n",
    "    with open(txt_data_path + file_name) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    data_ = filter_line_end(data)\n",
    "    print(\"Loaded data and filtered line endings\")\n",
    "    only_evo = get_evolutionary_from_all_data(data_, data_lim)\n",
    "    print(\"Extracted evolutionary data\")\n",
    "    res = group_aminoacids_together(only_evo, every_n = 21)\n",
    "    print(\"Grouped 21's together\")\n",
    "    return res\n",
    "\n",
    "def parse_primary_from_file(path, data_lim=None):\n",
    "    with open(txt_data_path + file_name) as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    data_ = filter_line_end(data)\n",
    "    print(\"Loaded data and filtered line endings\")\n",
    "    primary = get_primary_from_all_data(data_, data_lim)\n",
    "    print(\"Extracted primary data\")\n",
    "    le = load_file('le.joblib')\n",
    "    ohe = load_file('ohe.joblib')\n",
    "    primary_in_floats = [le.transform([_ for _ in c]) for c in primary]\n",
    "    primary_encoded = [ohe.transform(a.reshape(-1,1)).toarray() for a in primary_in_floats]\n",
    "    print(\"Encoded primary sequences\")\n",
    "    return primary_encoded\n",
    "\n",
    "def get_dih(protein_tertiary):\n",
    "    p = protein_tertiary\n",
    "    r = p.shape[0]\n",
    "    a_list = list(range(r))\n",
    "    the_list = np.array([a_list[slice(i, i+4)] for i in range(r - 4+1)])\n",
    "    slices = np.asarray(p[the_list], dtype=np.float32)\n",
    "    one_dih = np.array([dihedral(slice_) for slice_ in slices])\n",
    "    one_dih = np.insert(one_dih, 0, None)\n",
    "    one_dih = np.append(one_dih, [None,None])\n",
    "    return one_dih.reshape(-1,3)\n",
    "\n",
    "def save_file(data, path):\n",
    "    joblib.dump(data, path) \n",
    "    \n",
    "def load_file(path):\n",
    "    return joblib.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was used to generate the dihedral angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13024/13024 [00:00<00:00, 43820.84it/s]\n",
      "  0%|          | 0/13024 [00:00<?, ?it/s]/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n",
      " 79%|███████▉  | 10303/13024 [04:20<01:15, 36.18it/s]"
     ]
    }
   ],
   "source": [
    "# file_name = 'training_50'\n",
    "# # file_name = 'validation'\n",
    "# tertiary = parse_tertiary_from_file(txt_data_path + file_name)\n",
    "\n",
    "# dih = []\n",
    "# for protein in tqdm(tertiary):\n",
    "#     dih.append(get_dih(protein))\n",
    "# save_file(dih, file_name + '_dih.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_lim is important because so far the full 13000 proteins crashes my system with 16Gb of ram\n",
    "\n",
    "Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and filtered line endings\n",
      "Extracted primary data\n",
      "Encoded primary sequences\n",
      "Loaded data and filtered line endings\n",
      "Extracted evolutionary data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 13589.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped 21's together\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, (70, 20), 5000, (70, 21), 5000, (70, 3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'training_50'\n",
    "data_lim = 5000\n",
    "prim_train, evo_train, dih_train = (parse_primary_from_file(txt_data_path + file_name, data_lim), \n",
    "                                    parse_evolutionary_from_file(txt_data_path + file_name, data_lim),\n",
    "                                    load_file('./'+file_name+'_dih.joblib'))\n",
    "dih_train = dih_train[:data_lim]\n",
    "len(prim_train), prim_train[0].shape, len(evo_train), evo_train[0].shape, len(dih_train), dih_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and filtered line endings\n",
      "Extracted primary data\n",
      "Encoded primary sequences\n",
      "Loaded data and filtered line endings\n",
      "Extracted evolutionary data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224/224 [00:00<00:00, 9211.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped 21's together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(224, (269, 20), 224, (269, 21), 224, (269, 3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'validation'\n",
    "prim_valid, evo_valid, dih_valid = (parse_primary_from_file(txt_data_path + file_name), \n",
    "                                    parse_evolutionary_from_file(txt_data_path + file_name),\n",
    "                                    load_file('./'+ file_name + '_dih.joblib'))\n",
    "len(prim_valid), prim_valid[0].shape, len(evo_valid), evo_valid[0].shape, len(dih_valid), dih_valid[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the data and limit protein length. Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nans(a):\n",
    "    where_are_NaNs = np.isnan(a)\n",
    "    a[where_are_NaNs] = 0\n",
    "    return a\n",
    "\n",
    "def pad_array(array, max_len):\n",
    "    return np.asarray([np.pad(a, [(max_len - len(a), 0), (0,0)], mode='constant') for a in array], dtype=np.float32)\n",
    "\n",
    "def limit_length_and_pad(prim, evo, dih, max_length):\n",
    "    mask = np.array([len(el) for el in prim]) <= max_length\n",
    "    prim_lim, evo_lim, dih_lim = np.array(prim)[mask], np.array(evo)[mask], np.array(dih)[mask]\n",
    "#     max_ = np.max([len(a) for a in prim_lim])\n",
    "    prim_pad, evo_pad, dih_pad = pad_array(prim_lim, max_length), pad_array(evo_lim, max_length), pad_array(dih_lim, max_length)\n",
    "    return fix_nans(prim_pad), fix_nans(evo_pad), fix_nans(dih_pad)\n",
    "\n",
    "max_len = 200\n",
    "prim_, evo_, dih_ = limit_length_and_pad(prim_train, evo_train, dih_train, max_len)\n",
    "x_train, y_train = np.concatenate([prim_, evo_], axis=2), dih_\n",
    "prim_v, evo_v, dih_v = limit_length_and_pad(prim_valid, evo_valid, dih_valid, max_len)\n",
    "x_valid, y_valid = np.concatenate([prim_v, evo_v], axis=2), dih_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a simple regression task to check if it trains on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3053 samples, validate on 127 samples\n",
      "Epoch 1/40\n",
      "3053/3053 [==============================] - 32s 11ms/step - loss: 2.3215 - mean_absolute_error: 0.9678 - val_loss: 2.1291 - val_mean_absolute_error: 0.8712\n",
      "Epoch 2/40\n",
      "3053/3053 [==============================] - 31s 10ms/step - loss: 2.2601 - mean_absolute_error: 0.9201 - val_loss: 2.1089 - val_mean_absolute_error: 0.8566\n",
      "Epoch 3/40\n",
      " 192/3053 [>.............................] - ETA: 28s - loss: 2.3166 - mean_absolute_error: 0.9431"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Multiply, Lambda\n",
    "# from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# max_features = 1024\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Embedding(max_features, output_dim=256))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='tanh'))\n",
    "model.add(Lambda(lambda x: x*np.pi))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), batch_size=32, epochs=40)\n",
    "# score = model.evaluate(x_valid, y_valid, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
