{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "from final.model import Model\n",
    "from final.helpers import Helpers\n",
    "from final.datahandler import DataHandler\n",
    "from final.dihedralcalculator import DihedralCalculator\n",
    "\n",
    "import txt_data_utils.data_transformer as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA AND QUEUE CONFIGURATION ###\n",
    "data_path = \"/home/mikey/Data/ProteinNet/\"\n",
    "validation_casps = ['casp11']\n",
    "training_casps = ['casp11']\n",
    "training_percentages = [30, 50, 70]\n",
    "max_len = None # max len of the protein taken into account\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size= 32\n",
    "capacity=1000\n",
    "\n",
    "### MODEL CONFIGURATION ###\n",
    "include_evo = True\n",
    "\n",
    "model_type = 'cnn_big'\n",
    "dropout_rate = 0.1\n",
    "\n",
    "mode_a = 'regression' # regression or alphabet\n",
    "mode_b = 'vectors' # angles or vectors\n",
    "\n",
    "prediction_mode = mode_a + '_' + mode_b\n",
    "n_clusters = 50 # only needed when prediction mode == alphabet\n",
    "\n",
    "angularization_mode = None\n",
    "regularize_vectors = None\n",
    "if prediction_mode == 'regression_angles':\n",
    "    angularization_mode = 'cos'\n",
    "\n",
    "if prediction_mode == 'regression_vectors':\n",
    "    regularize_vectors = True # True or False, only works in the regression_vectors mode\n",
    "\n",
    "loss_mode = 'mae'\n",
    "n_angles = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mikey/Data/ProteinNet/casp11/validation/*']\n",
      "Training samples available 224\n",
      "Done\n",
      "padded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((224, 698, 41), (224, 698, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "data_handler = DataHandler(data_path=data_path,\n",
    "                           casps=validation_casps,\n",
    "                           num_epochs=1,\n",
    "                           mode='validation')\n",
    "\n",
    "ids, one_hot_primary, evolutionary, _, tertiary, ter_mask, pri_length, keep =\\\n",
    "    data_handler.generate_batches(batch_size = 1,\n",
    "                                  capacity = 1000,\n",
    "                                  max_protein_len = max_len)\n",
    "\n",
    "dihedral_calculator = DihedralCalculator()\n",
    "true_dihedrals = dihedral_calculator.dihedral_pipeline(tertiary, protein_length = tf.shape(one_hot_primary)[1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "    \n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "    try:\n",
    "        ids_v, ohp_v, evo_v, tert_v, ter_mask_v, dih_v = [], [], [], [], [], []\n",
    "        while not coord.should_stop(): \n",
    "            ids_, one_hot_primary_, evolutionary_, tertiary_, ter_mask_, pri_length_, keep_, true_dihedrals_ = sess.run([ids, one_hot_primary, evolutionary, tertiary, \n",
    "                                                                                                        ter_mask, pri_length, keep, true_dihedrals])\n",
    "        \n",
    "            ids_v.append(np.squeeze(ids_))\n",
    "            ohp_v.append(np.squeeze(one_hot_primary_))\n",
    "            evo_v.append(np.squeeze(evolutionary_))\n",
    "            tert_v.append(np.squeeze(tertiary_))\n",
    "            ter_mask_v.append(np.squeeze(ter_mask_))\n",
    "            dih_v.append(np.squeeze(true_dihedrals_))\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done')\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "        \n",
    "prim_v, evo_v, dih_v, mask_v = dt.limit_length_and_pad(ohp_v, evo_v, dih_v, ter_mask_v, max_length=None)\n",
    "x_valid, y_valid = np.concatenate([prim_v, evo_v], axis=2), dih_v\n",
    "x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mikey/Data/ProteinNet/casp11/training/30/*', '/home/mikey/Data/ProteinNet/casp11/training/50/*', '/home/mikey/Data/ProteinNet/casp11/training/70/*']\n",
      "Training samples available 88285\n",
      "Parameters: 775130\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# define the training data paths and how many epochs they should be queued for\n",
    "# by instantiating the DataHandler object that takes care of parsing\n",
    "data_handler = DataHandler(data_path=data_path,\n",
    "                           casps=training_casps,\n",
    "                           percentages=training_percentages, \n",
    "                           num_epochs=num_epochs,\n",
    "                           mode='training')\n",
    "\n",
    "# use DataHandler to generate batches of specific size \n",
    "# and optional limit on protein length\n",
    "# secondary structure is missing from the ProteinNet, thus the underscore\n",
    "ids, one_hot_primary, evolutionary, _, tertiary, ter_mask, pri_length, keep =\\\n",
    "    data_handler.generate_batches(batch_size = batch_size,\n",
    "                                  capacity = capacity,\n",
    "                                  max_protein_len = max_len)\n",
    "\n",
    "# convert euclidean coordinates to dihedral angles\n",
    "dihedral_calculator = DihedralCalculator()\n",
    "true_dihedrals = dihedral_calculator.dihedral_pipeline(tertiary, protein_length = tf.shape(one_hot_primary)[1])\n",
    "true_dihedrals = true_dihedrals[:,:,:n_angles]\n",
    "\n",
    "# set up placeholders with batch_size=None to be able to feed them with validation data\n",
    "# they fall onto default coming from the queue if nothing is fed through feed_dict\n",
    "true_dihedrals = tf.placeholder_with_default(true_dihedrals, shape=(None, None, n_angles))\n",
    "true_vectors = Helpers.ang_to_vec_tf(true_dihedrals)\n",
    "one_hot_primary = tf.placeholder_with_default(one_hot_primary, shape=(None, None, 20))\n",
    "evolutionary = tf.placeholder_with_default(evolutionary, shape=(None, None, 21))\n",
    "ter_mask = tf.placeholder_with_default(ter_mask, shape=(None, None))\n",
    "\n",
    "# build a model and get predicted output\n",
    "model = Model(n_angles=n_angles, \n",
    "              n_clusters=n_clusters,\n",
    "              output_mask=ter_mask,\n",
    "              model_type=model_type, \n",
    "              prediction_mode=prediction_mode,\n",
    "              dropout_rate=dropout_rate,\n",
    "              ang_mode=angularization_mode,\n",
    "              regularize_vectors=regularize_vectors,\n",
    "              loss_mode=loss_mode\n",
    "            )\n",
    "\n",
    "if include_evo:\n",
    "    input_data = tf.concat([one_hot_primary, evolutionary], axis=2)\n",
    "else:\n",
    "    input_data = one_hot_primary\n",
    "\n",
    "rad_pred_masked, vec_pred_masked = model.build_model(input_data)\n",
    "\n",
    "true_dihedrals_masked, true_vectors_masked = model.mask_other([true_dihedrals, true_vectors])\n",
    "\n",
    "loss, loss_vec = model.calculate_loss(true_dihedrals_masked, rad_pred_masked,\n",
    "                                      true_vectors_masked, vec_pred_masked)\n",
    "\n",
    "pcc = Helpers.pearson_tf(rad_pred_masked, true_dihedrals_masked)\n",
    "\n",
    "# learning rate placeholder for adaptive learning rate\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "# choose an optimizer to minimize the loss\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "try:\n",
    "    n_parameters = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "    print(\"Parameters:\", n_parameters)\n",
    "except:\n",
    "    print(\"Couldn't calculate the number of parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.40042 [[0.36059526 0.22162743]\n",
      " [0.5692388  0.4502187 ]]\n",
      "Validation loss: 0.33133882 [[0.319742   0.20129289]\n",
      " [0.45140687 0.35291362]]\n",
      "Validation PCC: [0.46040076, 0.5032341]\n",
      "Train loss: 0.3113002 [[0.30007192 0.2010552 ]\n",
      " [0.41446477 0.3296089 ]]\n",
      "Validation loss: 0.29613787 [[0.28904602 0.19997355]\n",
      " [0.3851328  0.31039906]]\n",
      "Validation PCC: [0.5687059, 0.5833979]\n",
      "Train loss: 0.29035965 [[0.28551248 0.19652791]\n",
      " [0.37348288 0.30591542]]\n",
      "Validation loss: 0.28172362 [[0.2817525  0.1809705 ]\n",
      " [0.36720082 0.29697064]]\n",
      "Validation PCC: [0.56943446, 0.6145757]\n",
      "Train loss: 0.27552235 [[0.27859658 0.16986962]\n",
      " [0.35866624 0.29495707]]\n",
      "Validation loss: 0.27157524 [[0.27470902 0.16747361]\n",
      " [0.35571575 0.28840253]]\n",
      "Validation PCC: [0.59819824, 0.6256365]\n",
      "Train loss: 0.26667032 [[0.2718317  0.16326924]\n",
      " [0.34436148 0.2872188 ]]\n",
      "Validation loss: 0.26461837 [[0.26865056 0.16417548]\n",
      " [0.3445286  0.2811188 ]]\n",
      "Validation PCC: [0.6169168, 0.6450863]\n",
      "Train loss: 0.25998354 [[0.26605296 0.15763278]\n",
      " [0.33440426 0.28184402]]\n",
      "Validation loss: 0.26031178 [[0.26508895 0.1615489 ]\n",
      " [0.33688015 0.27772918]]\n",
      "Validation PCC: [0.62438864, 0.65849394]\n",
      "Train loss: 0.25762478 [[0.26436657 0.15760523]\n",
      " [0.3305597  0.27796766]]\n",
      "Validation loss: 0.25745448 [[0.2628911  0.16020204]\n",
      " [0.33206028 0.27466452]]\n",
      "Validation PCC: [0.6305125, 0.65619904]\n",
      "Train loss: 0.25657237 [[0.26401863 0.15830563]\n",
      " [0.3278273  0.27613792]]\n",
      "Validation loss: 0.25494516 [[0.26124233 0.15886386]\n",
      " [0.3270419  0.27263257]]\n",
      "Validation PCC: [0.6356406, 0.6695145]\n",
      "Train loss: 0.25224486 [[0.25995553 0.1548193 ]\n",
      " [0.32149306 0.2727118 ]]\n",
      "Validation loss: 0.25467175 [[0.2603562  0.15835819]\n",
      " [0.327461   0.2725116 ]]\n",
      "Validation PCC: [0.63556933, 0.664487]\n",
      "Train loss: 0.24979538 [[0.25795844 0.15303017]\n",
      " [0.3171663  0.27102667]]\n",
      "Validation loss: 0.2518216 [[0.25988135 0.15672478]\n",
      " [0.32159394 0.26908636]]\n",
      "Validation PCC: [0.6358564, 0.67390114]\n",
      "Train loss: 0.25097233 [[0.25961453 0.15514453]\n",
      " [0.3173479  0.27178237]]\n",
      "Validation loss: 0.24986276 [[0.25809348 0.1557049 ]\n",
      " [0.3191628  0.26648986]]\n",
      "Validation PCC: [0.64390653, 0.68049586]\n",
      "Train loss: 0.2484381 [[0.2574371  0.1523331 ]\n",
      " [0.31501046 0.268972  ]]\n",
      "Validation loss: 0.25241974 [[0.2583115  0.15637287]\n",
      " [0.3226503  0.27234423]]\n",
      "Validation PCC: [0.6436959, 0.6801298]\n",
      "Train loss: 0.24862747 [[0.2574491  0.15349847]\n",
      " [0.31410873 0.2694535 ]]\n",
      "Validation loss: 0.24904127 [[0.2560617  0.15573707]\n",
      " [0.31781268 0.26655364]]\n",
      "Validation PCC: [0.64468896, 0.6771628]\n",
      "EPOCH. New learning rate: 0.00099\n",
      "Train loss: 0.24257126 [[0.2517179  0.14956784]\n",
      " [0.3050359  0.26396337]]\n",
      "Validation loss: 0.24708551 [[0.25474113 0.15404779]\n",
      " [0.31427157 0.26528153]]\n",
      "Validation PCC: [0.6482328, 0.6828487]\n",
      "Train loss: 0.24246524 [[0.25260997 0.14969528]\n",
      " [0.3034386  0.26411715]]\n",
      "Validation loss: 0.24597314 [[0.25334457 0.15360159]\n",
      " [0.31207195 0.2648744 ]]\n",
      "Validation PCC: [0.65349615, 0.691978]\n",
      "Train loss: 0.24325614 [[0.25326508 0.15163487]\n",
      " [0.30517623 0.26294822]]\n",
      "Validation loss: 0.24572524 [[0.25237173 0.15416418]\n",
      " [0.31152895 0.2648361 ]]\n",
      "Validation PCC: [0.6565111, 0.684478]\n",
      "Train loss: 0.23920034 [[0.24960393 0.14823255]\n",
      " [0.29931086 0.25965405]]\n",
      "Validation loss: 0.24412571 [[0.25177386 0.15329948]\n",
      " [0.3092648  0.26216465]]\n",
      "Validation PCC: [0.65709025, 0.6884846]\n",
      "Train loss: 0.2378585 [[0.24799938 0.1470484 ]\n",
      " [0.29748932 0.2588967 ]]\n",
      "Validation loss: 0.24440865 [[0.251768   0.15308921]\n",
      " [0.30973652 0.26304084]]\n",
      "Validation PCC: [0.6602368, 0.6937651]\n",
      "Train loss: 0.23934036 [[0.25037068 0.14892262]\n",
      " [0.2984196  0.2596485 ]]\n",
      "Validation loss: 0.24382815 [[0.25203875 0.15299138]\n",
      " [0.30769262 0.26258984]]\n",
      "Validation PCC: [0.65792894, 0.6912378]\n",
      "Train loss: 0.23572272 [[0.24618286 0.14632039]\n",
      " [0.29369146 0.25669613]]\n",
      "Validation loss: 0.24168587 [[0.24975735 0.15259573]\n",
      " [0.3050707  0.25931972]]\n",
      "Validation PCC: [0.6636073, 0.69712025]\n",
      "Train loss: 0.23944893 [[0.2508646  0.14979766]\n",
      " [0.29859647 0.25853688]]\n",
      "Validation loss: 0.24198484 [[0.24981272 0.15241131]\n",
      " [0.30495128 0.26076403]]\n",
      "Validation PCC: [0.6608613, 0.6941764]\n"
     ]
    }
   ],
   "source": [
    "learning_rate_decay = 0.99\n",
    "steps_to_print_after = 200\n",
    "init_learning_rate = 0.001\n",
    "\n",
    "if 'lstm' in model_type:\n",
    "    init_learning_rate = 0.001\n",
    "    steps_to_print_after = 200\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # important to call both of these, because \n",
    "    # otherwise can't specify num_epochs in string_input_producer\n",
    "    sess.run(init)\n",
    "    \n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "    try:\n",
    "        # we can't access information from the queue\n",
    "        # to know when an epoch ends, so we define our\n",
    "        # own step counter and calculate an validation loss every n steps\n",
    "        step = 1\n",
    "        \n",
    "        losses = []\n",
    "        loss_vecs = []\n",
    "        avg_losses = []\n",
    "        \n",
    "        while not coord.should_stop():        \n",
    "                \n",
    "            _, true_dihedrals_, rad_pred_masked_, loss_, loss_vec_ = sess.run([train_op, true_dihedrals, \n",
    "                                                                    rad_pred_masked, loss, loss_vec], \n",
    "                                                                    feed_dict={learning_rate: init_learning_rate})\n",
    "\n",
    "            losses.append(loss_)\n",
    "            loss_vecs.append(loss_vec_)\n",
    "            \n",
    "            if step % steps_to_print_after == 0:\n",
    "                avg_loss, avg_loss_vec =  np.mean(losses), np.mean(np.array(loss_vecs), axis=0)\n",
    "                avg_losses.append(avg_loss)\n",
    "                print(\"Train loss:\", avg_loss, avg_loss_vec)\n",
    "\n",
    "                losses = []\n",
    "                loss_vecs = []\n",
    "                \n",
    "                (true_dihedrals_masked_v, rad_pred_masked_v, \n",
    "                 loss_, loss_vec_) = sess.run([true_dihedrals_masked, rad_pred_masked, \n",
    "                                                   loss, loss_vec], \n",
    "                                                   feed_dict={\n",
    "                                                           one_hot_primary: prim_v,\n",
    "                                                           evolutionary: evo_v,\n",
    "                                                           true_dihedrals: dih_v[:,:,:n_angles],\n",
    "                                                           ter_mask: mask_v\n",
    "                                                          })\n",
    "                print(\"Validation loss:\", loss_, loss_vec_)\n",
    "                print(\"Validation PCC:\", Helpers.pearson_numpy(np.squeeze(true_dihedrals_masked_v)[:,:n_angles], \n",
    "                                                               np.array(rad_pred_masked_v)))\n",
    "            \n",
    "            if step * batch_size > data_handler.training_samples:\n",
    "                step = 0\n",
    "                init_learning_rate = init_learning_rate * learning_rate_decay\n",
    "                print(\"EPOCH. New learning rate:\", init_learning_rate)\n",
    "                \n",
    "            step += 1\n",
    "\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the avg losses over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get just the dihedral angles to see if they resemble how a ramachadran plot should look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    \n",
    "    true_dihedrals_, ohp, tert = sess.run([true_dihedrals, one_hot_primary, tertiary])\n",
    "    coord.request_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(true_dihedrals_v - pred_v)), loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_n = 0\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "phi, psi, omega = np.split(pred_v, 3, -1)\n",
    "plt.scatter(phi[:500], psi[:500], s=1, label='pred')\n",
    "\n",
    "phi, psi, omega = np.split(true_dihedrals_v, 3, -1)\n",
    "plt.scatter(phi[:500], psi[:500], s=1, label='true')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(omega[protein_n], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 # access trainable variables to see if they're training\n",
    "#                 variables_names = [v.name for v in tf.trainable_variables()]\n",
    "#                 values = sess.run(variables_names)\n",
    "#                 for k, v in zip(variables_names, values):\n",
    "#                     print(\"Variable: \", k)\n",
    "#                     print(\"Shape: \", v.shape)\n",
    "#                     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _coordinates(config, dihedrals):\n",
    "#     \"\"\" Converts dihedrals into full 3D structures. \"\"\"\n",
    "\n",
    "#     # converts dihedrals to points ready for reconstruction.\n",
    "#     points = dihedral_to_point(dihedrals) # [NUM_STEPS x NUM_DIHEDRALS, BATCH_SIZE, NUM_DIMENSIONS]\n",
    "             \n",
    "#     # converts points to final 3D coordinates.\n",
    "#     coordinates = point_to_coordinate(points, num_fragments=config['num_reconstruction_fragments'], \n",
    "#                                               parallel_iterations=config['num_reconstruction_parallel_iters']) \n",
    "#                   # [NUM_STEPS x NUM_DIHEDRALS, BATCH_SIZE, NUM_DIMENSIONS]\n",
    "\n",
    "#     return coordinates\n",
    "\n",
    "# def _drmsds(config, coordinates, targets, weights):\n",
    "#     \"\"\" Computes reduced weighted dRMSD loss (as specified by weights) \n",
    "#         between predicted tertiary structures and targets. \"\"\"\n",
    "                  \n",
    "#     # compute per structure dRMSDs\n",
    "#     drmsds = drmsd(coordinates, targets, weights, name='drmsds') # [BATCH_SIZE]\n",
    "\n",
    "#     # add to relevant collections for summaries, etc.\n",
    "#     if config['log_model_summaries']: tf.add_to_collection(config['name'] + '_drmsdss', drmsds)\n",
    "\n",
    "#     return drmsds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
