{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "from final.model import Model\n",
    "from final.helpers import Helpers\n",
    "from final.datahandler import DataHandler\n",
    "from final.dihedralcalculator import DihedralCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and filtered line endings\n",
      "Extracted primary data\n",
      "Encoded primary sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224/224 [00:00<00:00, 64870.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and filtered line endings\n",
      "Extracted evolutionary data\n",
      "Grouped 21's together\n",
      "Loaded data and filtered line endings\n",
      "Extracted mask data\n",
      "padded\n",
      "(224,) (269, 20)\n",
      "padded\n",
      "(224,) (269, 21)\n",
      "padded\n",
      "(224,) (269, 3)\n",
      "padded\n",
      "(224,) (269, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((224, 696, 41), (224, 696, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import txt_data_utils.data_loader as dl\n",
    "import txt_data_utils.data_transformer as dt\n",
    "# change this path to match the path of where you have the\n",
    "# training_50 and validation files\n",
    "txt_data_path = '/home/mikey/Data/ProteinNet/casp7_txt/'\n",
    "txt_utils_path = 'txt_data_utils/'\n",
    "\n",
    "max_len = None # max len of the protein taken into account\n",
    "n_angles = 3 # 3 if consider all angles or 2 if only phi and psi\n",
    "\n",
    "file_name = 'validation'\n",
    "prim_valid, evo_valid, dih_valid, mask_valid = (dl.parse_primary_from_file(txt_data_path + file_name), \n",
    "                                                dl.parse_evolutionary_from_file(txt_data_path + file_name),\n",
    "                                                dl.load_file('./'+ txt_utils_path + file_name + '_dih.joblib'),\n",
    "                                                dl.parse_mask_from_file(txt_data_path + file_name))\n",
    "\n",
    "prim_v, evo_v, dih_v, mask_v = dt.limit_length_and_pad(prim_valid, evo_valid, dih_valid, mask_valid, max_length=max_len)\n",
    "x_valid, y_valid = np.concatenate([prim_v, evo_v], axis=2), dih_v[:,:,:n_angles]\n",
    "\n",
    "x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mikey/Data/ProteinNet/casp7/training/30/*', '/home/mikey/Data/ProteinNet/casp7/training/50/*', '/home/mikey/Data/ProteinNet/casp7/training/70/*']\n",
      "Training samples available 38564\n",
      "WARNING:tensorflow:From /home/mikey/.local/lib/python3.6/site-packages/tensorflow/python/training/input.py:187: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/mikey/.local/lib/python3.6/site-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikey/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193654\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "data_path = \"/home/mikey/Data/ProteinNet/\"\n",
    "# casps = ['casp7', 'casp8', 'casp9', 'casp10', 'casp11', 'casp12']\n",
    "casps = ['casp7']\n",
    "training_percentages = [30, 50, 70]\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size= 32\n",
    "capacity=1000\n",
    "# min_after_dequeue=100\n",
    "\n",
    "max_protein_len = None\n",
    "include_evo = True\n",
    "apply_mask = True\n",
    "\n",
    "model_type = 'cnn_small'\n",
    "dropout_rate = 0.3\n",
    "prediction_mode = 'alphabet_angles'\n",
    "n_clusters = 50 # only needed when prediction mode == alphabet\n",
    "\n",
    "angularization_mode = None\n",
    "if prediction_mode == 'regression_angles':\n",
    "    angularization_mode = 'cos'\n",
    "\n",
    "regularize_vectors = False\n",
    "loss_mode = 'angular_mae'\n",
    "n_angles = 2\n",
    "\n",
    "# define the training data paths and how many epochs they should be queued for\n",
    "# by instantiating the DataHandler object that takes care of parsing\n",
    "data_handler = DataHandler(data_path=data_path,\n",
    "                           casps=casps,\n",
    "                           percentages=training_percentages, \n",
    "                           num_epochs=num_epochs)\n",
    "\n",
    "# use DataHandler to generate batches of specific size \n",
    "# and optional limit on protein length\n",
    "# secondary structure is missing from the ProteinNet, thus the underscore\n",
    "ids, one_hot_primary, evolutionary, _, tertiary, ter_mask, pri_length, keep =\\\n",
    "    data_handler.generate_batches(batch_size = batch_size,\n",
    "                                  capacity = capacity,\n",
    "                                  max_protein_len = max_protein_len)\n",
    "# if max_protein_len:\n",
    "#     ids, one_hot_primary, evolutionary, tertiary, ter_mask, pri_length =\\\n",
    "#         helpers.mask_all([ids, one_hot_primary, evolutionary, tertiary, ter_mask, pri_length], keep, axis=0)\n",
    "\n",
    "# convert euclidean coordinates to dihedral angles\n",
    "dihedral_calculator = DihedralCalculator()\n",
    "true_dihedrals = dihedral_calculator.dihedral_pipeline(tertiary, protein_length = tf.shape(one_hot_primary)[1])\n",
    "true_dihedrals = true_dihedrals[:,:,:n_angles]\n",
    "\n",
    "# set up placeholders with batch_size=None to be able to feed them with validation data\n",
    "# they fall onto default coming from the queue if nothing is fed through feed_dict\n",
    "true_dihedrals = tf.placeholder_with_default(true_dihedrals, shape=(None, None, n_angles))\n",
    "true_vectors = Helpers.ang_to_vec_tf(true_dihedrals)\n",
    "one_hot_primary = tf.placeholder_with_default(one_hot_primary, shape=(None, None, 20))\n",
    "evolutionary = tf.placeholder_with_default(evolutionary, shape=(None, None, 21))\n",
    "ter_mask = tf.placeholder_with_default(ter_mask, shape=(None, None))\n",
    "\n",
    "if include_evo:\n",
    "    input_data = tf.concat([one_hot_primary, evolutionary], axis=2)\n",
    "else:\n",
    "    input_data = one_hot_primary\n",
    "\n",
    "# build a model and get predicted output\n",
    "model = Model(n_angles=n_angles, \n",
    "              n_clusters=n_clusters,\n",
    "              output_mask=ter_mask,\n",
    "              model_type=model_type, \n",
    "              prediction_mode=prediction_mode,\n",
    "              dropout_rate=dropout_rate,\n",
    "              ang_mode=angularization_mode,\n",
    "              regularize_vectors=regularize_vectors,\n",
    "              loss_mode=loss_mode\n",
    "            )\n",
    "\n",
    "rad_pred_masked, vec_pred_masked = model.build_model(input_data)\n",
    "\n",
    "true_dihedrals_masked, true_vectors_masked = model.mask_other([true_dihedrals, true_vectors])\n",
    "\n",
    "loss, loss_vec = model.calculate_loss(true_dihedrals_masked, rad_pred_masked,\n",
    "                                      true_vectors_masked, vec_pred_masked)\n",
    "\n",
    "pcc = Helpers.pearson_tf(rad_pred_masked, true_dihedrals_masked)\n",
    "\n",
    "# learning rate placeholder for adaptive learning rate\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "# choose an optimizer to minimize the loss\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "try:\n",
    "    n_parameters = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "    print(n_parameters)\n",
    "except:\n",
    "    print(\"Couldn't calculate the number of parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-a1ccbff78411>:15: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Train loss: 1.0536946 [0.6118701 1.4955196]\n",
      "Validation loss: 1.035671 [0.5821547 1.4891874]\n",
      "Validation PCC: [0.015395108, 0.054869775]\n",
      "Train loss: 0.97628784 [0.5853456 1.3672304]\n",
      "Validation loss: 0.7048963 [0.5226411 0.8871514]\n",
      "Validation PCC: [0.4025742, 0.46565634]\n",
      "Train loss: 0.6384111 [0.4989848 0.7778377]\n",
      "Validation loss: 0.59608877 [0.4801363  0.71204126]\n",
      "Validation PCC: [0.5331576, 0.6051885]\n"
     ]
    }
   ],
   "source": [
    "learning_rate_decay = 0.99\n",
    "steps_to_print_after = 250\n",
    "init_learning_rate = 0.001\n",
    "\n",
    "if 'lstm' in model_type:\n",
    "    init_learning_rate = 0.001\n",
    "    steps_to_print_after = 200\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # important to call both of these, because \n",
    "    # otherwise can't specify num_epochs in string_input_producer\n",
    "    sess.run(init)\n",
    "    \n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "    try:\n",
    "        # we can't access information from the queue\n",
    "        # to know when an epoch ends, so we define our\n",
    "        # own step counter and calculate an validation loss every n steps\n",
    "        step = 1\n",
    "        \n",
    "        losses = []\n",
    "        loss_vecs = []\n",
    "        avg_losses = []\n",
    "        \n",
    "        while not coord.should_stop():        \n",
    "                \n",
    "            _, true_dihedrals_, rad_pred_masked_, loss_, loss_vec_ = sess.run([train_op, true_dihedrals, \n",
    "                                                                    rad_pred_masked, loss, loss_vec], \n",
    "                                                                    feed_dict={learning_rate: init_learning_rate})\n",
    "\n",
    "            losses.append(loss_)\n",
    "            loss_vecs.append(loss_vec_)\n",
    "            if step % steps_to_print_after == 0:\n",
    "                avg_loss, avg_loss_vec =  np.mean(losses), np.mean(np.array(loss_vecs), axis=0)\n",
    "                avg_losses.append(avg_loss)\n",
    "                print(\"Train loss:\", avg_loss, avg_loss_vec)\n",
    "\n",
    "                losses = []\n",
    "                loss_vecs = []\n",
    "                \n",
    "                (true_dihedrals_masked_v, rad_pred_masked_v, \n",
    "                 loss_, loss_vec_) = sess.run([true_dihedrals_masked, rad_pred_masked, \n",
    "                                                   loss, loss_vec], \n",
    "                                                   feed_dict={\n",
    "                                                           one_hot_primary: prim_v,\n",
    "                                                           evolutionary: evo_v,\n",
    "                                                           true_dihedrals: dih_v[:,:,:n_angles],\n",
    "                                                           ter_mask: mask_v\n",
    "                                                          })\n",
    "                print(\"Validation loss:\", loss_, loss_vec_)\n",
    "                print(\"Validation PCC:\", Helpers.pearson_numpy(np.squeeze(true_dihedrals_masked_v)[:,:n_angles], \n",
    "                                                               np.array(rad_pred_masked_v)))\n",
    "            \n",
    "            if step * batch_size > data_handler.training_samples:\n",
    "                step = 0\n",
    "                init_learning_rate = init_learning_rate * learning_rate_decay\n",
    "                print(\"EPOCH. New learning rate:\", init_learning_rate)\n",
    "                \n",
    "            step += 1\n",
    "\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the avg losses over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get just the dihedral angles to see if they resemble how a ramachadran plot should look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    \n",
    "    true_dihedrals_, ohp, tert = sess.run([true_dihedrals, one_hot_primary, tertiary])\n",
    "    coord.request_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(true_dihedrals_v - pred_v)), loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_n = 0\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "phi, psi, omega = np.split(pred_v, 3, -1)\n",
    "plt.scatter(phi[:500], psi[:500], s=1, label='pred')\n",
    "\n",
    "phi, psi, omega = np.split(true_dihedrals_v, 3, -1)\n",
    "plt.scatter(phi[:500], psi[:500], s=1, label='true')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(omega[protein_n], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 # access trainable variables to see if they're training\n",
    "#                 variables_names = [v.name for v in tf.trainable_variables()]\n",
    "#                 values = sess.run(variables_names)\n",
    "#                 for k, v in zip(variables_names, values):\n",
    "#                     print(\"Variable: \", k)\n",
    "#                     print(\"Shape: \", v.shape)\n",
    "#                     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _coordinates(config, dihedrals):\n",
    "#     \"\"\" Converts dihedrals into full 3D structures. \"\"\"\n",
    "\n",
    "#     # converts dihedrals to points ready for reconstruction.\n",
    "#     points = dihedral_to_point(dihedrals) # [NUM_STEPS x NUM_DIHEDRALS, BATCH_SIZE, NUM_DIMENSIONS]\n",
    "             \n",
    "#     # converts points to final 3D coordinates.\n",
    "#     coordinates = point_to_coordinate(points, num_fragments=config['num_reconstruction_fragments'], \n",
    "#                                               parallel_iterations=config['num_reconstruction_parallel_iters']) \n",
    "#                   # [NUM_STEPS x NUM_DIHEDRALS, BATCH_SIZE, NUM_DIMENSIONS]\n",
    "\n",
    "#     return coordinates\n",
    "\n",
    "# def _drmsds(config, coordinates, targets, weights):\n",
    "#     \"\"\" Computes reduced weighted dRMSD loss (as specified by weights) \n",
    "#         between predicted tertiary structures and targets. \"\"\"\n",
    "                  \n",
    "#     # compute per structure dRMSDs\n",
    "#     drmsds = drmsd(coordinates, targets, weights, name='drmsds') # [BATCH_SIZE]\n",
    "\n",
    "#     # add to relevant collections for summaries, etc.\n",
    "#     if config['log_model_summaries']: tf.add_to_collection(config['name'] + '_drmsdss', drmsds)\n",
    "\n",
    "#     return drmsds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
