{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "NUM_AAS = 20\n",
    "NUM_DIMENSIONS = 3\n",
    "\n",
    "def masking_matrix(mask, name=None):\n",
    "    \"\"\" Constructs a masking matrix to zero out pairwise distances due to missing residues or padding. \n",
    "\n",
    "    Args:\n",
    "        mask: 0/1 vector indicating whether a position should be masked (0) or not (1)\n",
    "\n",
    "    Returns:\n",
    "        A square matrix with all 1s except for rows and cols whose corresponding indices in mask are set to 0.\n",
    "        [MAX_SEQ_LENGTH, MAX_SEQ_LENGTH]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'masking_matrix', [mask]) as scope:\n",
    "        mask = tf.convert_to_tensor(mask, name='mask')\n",
    "\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        base = tf.ones([tf.size(mask), tf.size(mask)])\n",
    "        matrix_mask = base * mask * tf.transpose(mask)\n",
    "\n",
    "        return matrix_mask\n",
    "        \n",
    "def read_protein(filename_queue, max_length, num_evo_entries=21, name=None):\n",
    "    \"\"\" Reads and parses a ProteinNet TF Record. \n",
    "\n",
    "        Primary sequences are mapped onto 20-dimensional one-hot vectors.\n",
    "        Evolutionary sequences are mapped onto num_evo_entries-dimensional real-valued vectors.\n",
    "        Secondary structures are mapped onto ints indicating one of 8 class labels.\n",
    "        Tertiary coordinates are flattened so that there are 3 times as many coordinates as \n",
    "        residues.\n",
    "\n",
    "        Evolutionary, secondary, and tertiary entries are optional.\n",
    "\n",
    "    Args:\n",
    "        filename_queue: TF queue for reading files\n",
    "        max_length:     Maximum length of sequence (number of residues) [MAX_LENGTH]. Not a \n",
    "                        TF tensor and is thus a fixed value.\n",
    "\n",
    "    Returns:\n",
    "        id: string identifier of record\n",
    "        one_hot_primary: AA sequence as one-hot vectors\n",
    "        evolutionary: PSSM sequence as vectors\n",
    "        secondary: DSSP sequence as int class labels\n",
    "        tertiary: 3D coordinates of structure\n",
    "        matrix_mask: Masking matrix to zero out pairwise distances in the masked regions\n",
    "        pri_length: Length of amino acid sequence\n",
    "        keep: True if primary length is less than or equal to max_length\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'read_protein', []) as scope:\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "        context, features = tf.parse_single_sequence_example(serialized_example,\n",
    "                                context_features={'id': tf.FixedLenFeature((1,), tf.string)},\n",
    "                                sequence_features={\n",
    "                                    'primary':      tf.FixedLenSequenceFeature((1,),               tf.int64),\n",
    "                                    'evolutionary': tf.FixedLenSequenceFeature((num_evo_entries,), tf.float32, allow_missing=True),\n",
    "                                    'secondary':    tf.FixedLenSequenceFeature((1,),               tf.int64,   allow_missing=True),\n",
    "                                    'tertiary':     tf.FixedLenSequenceFeature((NUM_DIMENSIONS,),  tf.float32, allow_missing=True),\n",
    "                                    'mask':         tf.FixedLenSequenceFeature((1,),               tf.float32, allow_missing=True)})\n",
    "        id_ = context['id'][0]\n",
    "        primary =   tf.to_int32(features['primary'][:, 0])\n",
    "        evolutionary =          features['evolutionary']\n",
    "        secondary = tf.to_int32(features['secondary'][:, 0])\n",
    "        tertiary =              features['tertiary']\n",
    "        mask =                  features['mask'][:, 0]\n",
    "\n",
    "        pri_length = tf.size(primary)\n",
    "        keep = pri_length <= max_length\n",
    "\n",
    "        one_hot_primary = tf.one_hot(primary, NUM_AAS)\n",
    "\n",
    "        # Generate tertiary masking matrix--if mask is missing then assume all residues are present\n",
    "        mask = tf.cond(tf.not_equal(tf.size(mask), 0), lambda: mask, lambda: tf.ones([pri_length]))\n",
    "        ter_mask = masking_matrix(mask, name='ter_mask')        \n",
    "\n",
    "        return id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dihedral(p):\n",
    "    \"\"\"Praxeolitic formula\n",
    "    1 sqrt, 1 cross product\"\"\"\n",
    "    p0 = p[0]\n",
    "    p1 = p[1]\n",
    "    p2 = p[2]\n",
    "    p3 = p[3]\n",
    "\n",
    "    b0 = -1.0*(p1 - p0)\n",
    "    b1 = p2 - p1\n",
    "    b2 = p3 - p2\n",
    "\n",
    "    # normalize b1 so that it does not influence magnitude of vector\n",
    "    # rejections that come next\n",
    "    b1 /= np.linalg.norm(b1)\n",
    "\n",
    "    # vector rejections\n",
    "    # v = projection of b0 onto plane perpendicular to b1\n",
    "    #   = b0 minus component that aligns with b1\n",
    "    # w = projection of b2 onto plane perpendicular to b1\n",
    "    #   = b2 minus component that aligns with b1\n",
    "    v = b0 - np.dot(b0, b1)*b1\n",
    "    w = b2 - np.dot(b2, b1)*b1\n",
    "\n",
    "    # angle between v and w in a plane is the torsion angle\n",
    "    # v and w may not be normalized but that's fine since tan is y/x\n",
    "    x = np.dot(v, w)\n",
    "    y = np.dot(np.cross(b1, v), w)\n",
    "    return np.degrees(np.arctan2(y, x))\n",
    "\n",
    "def tf_rad2deg(rad):\n",
    "    pi_on_180 = 0.017453292519943295\n",
    "    return rad / pi_on_180\n",
    "\n",
    "# takes 1 dimensional tensor and outputs an angle\n",
    "def dihedral_tf1(p):\n",
    "    p0 = tf.gather(p, 0)\n",
    "    p1 = tf.gather(p, 1)\n",
    "    p2 = tf.gather(p, 2)\n",
    "    p3 = tf.gather(p, 3)\n",
    "    \n",
    "    b0 = -1.0 * (tf.subtract(p1, p0))\n",
    "    b1 = tf.subtract(p2, p1)\n",
    "    b2 = tf.subtract(p3, p2)\n",
    "    \n",
    "    b1 = tf.divide(b1, tf.norm(b1))\n",
    "    \n",
    "    v = tf.subtract(b0, tf.multiply(tf.tensordot(b0, b1, 1), b1))\n",
    "    w = tf.subtract(b2, tf.multiply(tf.tensordot(b2, b1, 1), b1))\n",
    "    \n",
    "    x = tf.tensordot(v, w, 1)\n",
    "    y = tf.tensordot(tf.cross(b1, v), w, 1)\n",
    "    \n",
    "    return tf_rad2deg(tf.atan2(y,x))\n",
    "\n",
    "# takes 2 dimensional tensor (K, 4) and outputs K angles\n",
    "def dihedral_tf2(p):\n",
    "    p0 = tf.gather(p, 0, axis=1)\n",
    "    p1 = tf.gather(p, 1, axis=1)\n",
    "    p2 = tf.gather(p, 2, axis=1)\n",
    "    p3 = tf.gather(p, 3, axis=1)\n",
    "    \n",
    "    b0 = -1.0 * (tf.subtract(p1, p0))\n",
    "    b1 = tf.subtract(p2, p1)\n",
    "    b2 = tf.subtract(p3, p2)\n",
    "    \n",
    "    b1 = tf.divide(b1, tf.norm(b1, axis=0))\n",
    "    \n",
    "    v = tf.subtract(b0, tf.multiply(tf.tensordot(b0, b1, 2), b1))\n",
    "    w = tf.subtract(b2, tf.multiply(tf.tensordot(b2, b1, 2), b1))\n",
    "    \n",
    "    x = tf.reduce_sum( tf.multiply( v, w ), 1, keepdims=True )\n",
    "    y = tf.reduce_sum( tf.multiply( tf.cross(b1, v), w ), 1, keepdims=True )\n",
    "\n",
    "    return tf_rad2deg(tf.atan2(y,x))\n",
    "\n",
    "# takes a 3 dimensional tensor (N, K, 4) and outputs (N,K) angles\n",
    "def dihedral_tf3(p):\n",
    "    p0 = tf.gather(p, 0, axis=2)\n",
    "    p1 = tf.gather(p, 1, axis=2)\n",
    "    p2 = tf.gather(p, 2, axis=2)\n",
    "    p3 = tf.gather(p, 3, axis=2)\n",
    "    \n",
    "    b0 = -1.0 * (tf.subtract(p1, p0))\n",
    "    b1 = tf.subtract(p2, p1)\n",
    "    b2 = tf.subtract(p3, p2)\n",
    "    \n",
    "    b1 = tf.divide(b1, tf.expand_dims(tf.norm(b1, axis=1), axis=1))\n",
    "    \n",
    "    v = tf.subtract(b0, tf.einsum('b,bij->bij', tf.einsum('bij,bij->b', b0, b1), b1))\n",
    "    w = tf.subtract(b2, tf.einsum('b,bij->bij', tf.einsum('bij,bij->b', b2, b1), b1))\n",
    "    \n",
    "    x = tf.reduce_sum( tf.multiply( v, w ), 2, keepdims=True )\n",
    "    y = tf.reduce_sum( tf.multiply( tf.cross(b1, v), w ), 2, keepdims=True )\n",
    "\n",
    "    return tf_rad2deg(tf.atan2(y,x))\n",
    "\n",
    "def slice_tf(tensor):\n",
    "    return tf.slice(tensor, (0,0), (4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 3)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    out = sess.run([slice_tf(test1)])\n",
    "    \n",
    "np.array(out).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need 4 coordinates to calculate an angle but the proteins are organized in 3x3 matricies. Thus how do we calculate an angle. Take 3 angles from 1 aminoacid and 1 angle from the next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1shape (7, 3)\n",
      "(2445, 4) (2448, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Tensor 'Gather:0' shape=(2445, 4, 3) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"Gather:0\", shape=(2445, 4, 3), dtype=float32) is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 282\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    283\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3477\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3478\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3556\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3557\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3558\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor Tensor(\"Gather:0\", shape=(2445, 4, 3), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-445-e0a9a9cd34fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mp1_tf_stacked_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp1_tf_stacked_full_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle1_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle2_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle3_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp1_tf_stacked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp1_tf_stacked_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mdihedral\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1_tf_stacked_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1_tf_stacked_full_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle1_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle2_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle3_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1125\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \"\"\"\n\u001b[0;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \"\"\"\n\u001b[0;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[1;32m--> 289\u001b[1;33m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[0;32m    290\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mValueError\u001b[0m: Fetch argument <tf.Tensor 'Gather:0' shape=(2445, 4, 3) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"Gather:0\", shape=(2445, 4, 3), dtype=float32) is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "\n",
    "p1 = np.array([[\n",
    "                [ 1,           0,         0     ],\n",
    "                [ 0,           0,         0     ],\n",
    "                [ 0,           0,         1     ],\n",
    "                [ 0.999999,    0.000001,  1     ],\n",
    "                [ 0.999999,    0.000001,  1     ],\n",
    "                [ 0.999999,    0.000001,  1     ],\n",
    "                [ 0.999999,    0.000001,  1     ]\n",
    "            ],[\n",
    "                [ 1,           0,         0     ],\n",
    "                [ 0,           0,         0     ],\n",
    "                [ 0,           0,         1     ],\n",
    "                [ 0.999999,    0.000001,  1     ],\n",
    "                [ 0.999999,    0.000001,  1     ],\n",
    "                [ 0.999999,    0.000001,  1     ],\n",
    "                [ 0.999999,    0.000001,  1     ]\n",
    "            ]])\n",
    "print(\"p1shape\", p1[0].shape)\n",
    "p1_tf = tf.convert_to_tensor(p1[0])\n",
    "test1 = test[0]\n",
    "test_full\n",
    "\n",
    "r = test1.shape[0]\n",
    "n = 4\n",
    "a_list = list(range(r))\n",
    "the_list1 = np.array([a_list[slice(i, i+n)] for i in range(r - n+1)])\n",
    "print(the_list1.shape, test1.shape)\n",
    "\n",
    "r = test_full.shape[1]\n",
    "n = 4\n",
    "a_list = list(range(r))\n",
    "the_list2 = [a_list[slice(i, i+n)] for i in range(r - n+1)]\n",
    "the_list2 = np.array([the_list2 for _ in range(test_full.shape[0])])\n",
    "the_list2 = the_list2\n",
    "# print(the_list2.shape, test_full.shape)\n",
    "# for i in range(len(r) - n + 1):\n",
    "#     r[i: i + n]\n",
    "\n",
    "p1_tf_stacked_full = tf.stack(tf.gather(test_full, the_list1, axis=1))\n",
    "\n",
    "angle1 = dihedral_tf1(p1_tf)\n",
    "angle2 = dihedral_tf2(p1_tf_stacked)\n",
    "angle3 = dihedral_tf3(p1_tf_stacked_full)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    p1_tf_stacked_, p1_tf_stacked_full_, angle1_, angle2_, angle3_ = sess.run([p1_tf_stacked, p1_tf_stacked_full, angle1, angle2, angle3])\n",
    "\n",
    "dihedral(p1[0]), np.array(p1_tf_stacked_).shape, np.array(p1_tf_stacked_full_).shape, angle1_, angle2_.shape, angle3_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 1]\n",
      "  [1 2]\n",
      "  [2 3]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [2 3]\n",
      "  [3 4]\n",
      "  [4 5]]\n",
      "\n",
      " [[2 3]\n",
      "  [3 4]\n",
      "  [4 5]\n",
      "  [5 6]]]\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[0, 1],[1, 2],[2, 3],[3, 4],[4, 5],[5, 6]])\n",
    "\n",
    "r = array.shape[0]\n",
    "n = 4\n",
    "a_list = list(range(r))\n",
    "the_list = np.array([a_list[slice(i, i+n)] for i in range(r - n+1)])\n",
    "\n",
    "array[the_list].shape\n",
    "\n",
    "print(array[the_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([141.95627], dtype=float32), array([141.95627], dtype=float32))"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle2_[36], angle3_[0][36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2445, 3), (2445, 3), (2, 2445, 3), (2, 2445, 3))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle2_[0].shape, angle2_[1].shape, angle3_[0].shape, angle3_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-756.6741, -756.6741], dtype=float32)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('bij,bij->b', angle3_[0], angle3_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_epochs = 1\n",
    "a_path = r'C:\\Users\\Michal\\Desktop\\ITU NLP\\casp7\\training\\30\\*'\n",
    "# init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())\n",
    "base_names = glob.glob(a_path)\n",
    "base_tensor = tf.convert_to_tensor(base_names[:1])\n",
    "file_queue = tf.train.string_input_producer(\n",
    "    base_tensor,\n",
    "    num_epochs=num_epochs,\n",
    "    shuffle=False # Note: must set shuffle to False\n",
    ")\n",
    "\n",
    "res = read_protein(file_queue, max_length=1000)\n",
    "id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep = res\n",
    "# one_hot_primary = tf.slice(one_hot_primary, 0, 10)\n",
    "\n",
    "lstm_units = 5\n",
    "batch_size=8\n",
    "capacity=1000\n",
    "min_after_dequeue=100\n",
    "\n",
    "## I couldn't make shuffle batch work\n",
    "## because it doesn't have the dynamic padding included\n",
    "# ids, data, length = tf.train.shuffle_batch(\n",
    "#       [id_, one_hot_primary, pri_length], \n",
    "#       batch_size=batch_size, \n",
    "#       capacity=capacity,\n",
    "#       min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "# dynamic pad makes sure that the length of the proteins\n",
    "# is padded to the longest protein in the batch\n",
    "ids, data, labels, length = tf.train.batch(\n",
    "      [id_, one_hot_primary, tertiary, pri_length], \n",
    "      batch_size=batch_size, \n",
    "      capacity=capacity, \n",
    "      dynamic_pad=True\n",
    "    )\n",
    "\n",
    "protein_length = tf.gather(tf.shape(data), 1)\n",
    "protein_euc_length = tf.gather(tf.shape(labels), 1)\n",
    "\n",
    "# r = test1.shape[1]\n",
    "# n = 4\n",
    "# a_list = list(range(r))\n",
    "# the_list1 = np.array([a_list[slice(i, i+n)] for i in range(r - n+1)])\n",
    "the_list = tf.placeholder(tf.int32, shape=(None, 4))\n",
    "\n",
    "# tile_test = tf.tile([tf.reduce_max(length)], (batch_size,))\n",
    "p1_tf_stacked_full = tf.stack(tf.gather(labels, the_list, axis=1))\n",
    "angle3 = dihedral_tf3(p1_tf_stacked_full)\n",
    "\n",
    "padding = tf.constant([[0, 0], [0,3], [0,0]])\n",
    "angle3 = tf.pad(angle3, padding)\n",
    "\n",
    "angle3_shape = tf.gather(tf.shape(angle3), [0,1])\n",
    "angle3 = tf.reshape(angle3, shape=angle3_shape)\n",
    "angle3 = tf.reshape(angle3, shape=(tf.gather(angle3_shape, 0), protein_length, 3))\n",
    "\n",
    "\n",
    "# seq_length = tf.reduce_max(length)\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=cell,\n",
    "    cell_bw=cell,\n",
    "    dtype=tf.float32,\n",
    "    inputs=data)\n",
    "outputs_conc = tf.concat(outputs, 2)\n",
    "pred = tf.layers.dense(outputs_conc, 3, activation=tf.nn.tanh, use_bias=False)\n",
    "# loss = tf.losses.mean_squared_error(labels=labels, predictions=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein_euc_length 1143\n",
      "protein_length 381\n",
      "(1140, 4)\n",
      "(8, 381, 3)\n",
      "protein_euc_length 1167\n",
      "protein_length 389\n",
      "(1164, 4)\n",
      "(8, 389, 3)\n",
      "protein_euc_length 1095\n",
      "protein_length 365\n",
      "(1092, 4)\n",
      "(8, 365, 3)\n",
      "protein_euc_length 831\n",
      "protein_length 277\n",
      "(828, 4)\n",
      "(8, 277, 3)\n",
      "protein_euc_length 921\n",
      "protein_length 307\n",
      "(918, 4)\n",
      "(8, 307, 3)\n",
      "protein_euc_length 2574\n",
      "protein_length 858\n",
      "(2571, 4)\n",
      "(8, 858, 3)\n",
      "protein_euc_length 1785\n",
      "protein_length 595\n",
      "(1782, 4)\n",
      "(8, 595, 3)\n",
      "protein_euc_length 1497\n",
      "protein_length 499\n",
      "(1494, 4)\n",
      "(8, 499, 3)\n",
      "protein_euc_length 825\n",
      "protein_length 275\n",
      "(822, 4)\n",
      "(8, 275, 3)\n",
      "protein_euc_length 1623\n",
      "protein_length 541\n",
      "(1620, 4)\n",
      "(8, 541, 3)\n",
      "protein_euc_length 969\n",
      "protein_length 323\n",
      "(966, 4)\n",
      "(8, 323, 3)\n",
      "protein_euc_length 1302\n",
      "protein_length 434\n",
      "(1299, 4)\n",
      "(8, 434, 3)\n",
      "protein_euc_length 1824\n",
      "protein_length 608\n",
      "(1821, 4)\n",
      "(8, 608, 3)\n",
      "protein_euc_length 804\n",
      "protein_length 268\n",
      "(801, 4)\n",
      "(8, 268, 3)\n",
      "protein_euc_length 1512\n",
      "protein_length 504\n",
      "(1509, 4)\n",
      "(8, 504, 3)\n",
      "protein_euc_length 930\n",
      "protein_length 310\n",
      "(927, 4)\n",
      "(8, 310, 3)\n",
      "protein_euc_length 660\n",
      "protein_length 220\n",
      "(657, 4)\n",
      "(8, 220, 3)\n",
      "protein_euc_length 981\n",
      "protein_length 327\n",
      "(978, 4)\n",
      "(8, 327, 3)\n",
      "protein_euc_length 822\n",
      "protein_length 274\n",
      "(819, 4)\n",
      "(8, 274, 3)\n",
      "protein_euc_length 1485\n",
      "protein_length 495\n",
      "(1482, 4)\n",
      "(8, 495, 3)\n",
      "protein_euc_length 1224\n",
      "protein_length 408\n",
      "(1221, 4)\n",
      "(8, 408, 3)\n",
      "protein_euc_length 1005\n",
      "protein_length 335\n",
      "(1002, 4)\n",
      "(8, 335, 3)\n",
      "protein_euc_length 873\n",
      "protein_length 291\n",
      "(870, 4)\n",
      "(8, 291, 3)\n",
      "protein_euc_length 1533\n",
      "protein_length 511\n",
      "(1530, 4)\n",
      "(8, 511, 3)\n",
      "protein_euc_length 552\n",
      "protein_length 184\n",
      "(549, 4)\n",
      "(8, 184, 3)\n",
      "protein_euc_length 1416\n",
      "protein_length 472\n",
      "(1413, 4)\n",
      "(8, 472, 3)\n",
      "protein_euc_length 1560\n",
      "protein_length 520\n",
      "(1557, 4)\n",
      "(8, 520, 3)\n",
      "protein_euc_length 1320\n",
      "protein_length 440\n",
      "(1317, 4)\n",
      "(8, 440, 3)\n",
      "protein_euc_length 1044\n",
      "protein_length 348\n",
      "(1041, 4)\n",
      "(8, 348, 3)\n",
      "protein_euc_length 2448\n",
      "protein_length 816\n",
      "(2445, 4)\n",
      "(8, 816, 3)\n",
      "protein_euc_length 1728\n",
      "protein_length 576\n",
      "(1725, 4)\n",
      "(8, 576, 3)\n",
      "protein_euc_length 672\n",
      "protein_length 224\n",
      "(669, 4)\n",
      "(8, 224, 3)\n",
      "Done training for 1 epochs, 0 steps.\n"
     ]
    }
   ],
   "source": [
    "num_examples = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "#     batch = tf.train.shuffle_batch([id_], 10, 200, 100)\n",
    "    try:\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            start_time = time.time()\n",
    "            \n",
    "            h = sess.partial_run_setup([protein_euc_length, protein_length, angle3, p1_tf_stacked_full, labels], [the_list])\n",
    "        \n",
    "            protein_euc_length_, protein_length_ = sess.partial_run(h, [protein_euc_length, protein_length])\n",
    "#             protein_euc_length_ = protein_euc_length_\n",
    "#             protein_length_ = protein_length_[0]\n",
    "            print('protein_euc_length', protein_euc_length_)\n",
    "            print('protein_length', protein_length_)\n",
    "        \n",
    "#             r = array.shape[0]\n",
    "            n = 4\n",
    "            a_list = list(range(protein_euc_length_))\n",
    "            the_list_ = np.array([a_list[slice(i, i+n)] for i in range(protein_euc_length_ - n+1)])\n",
    "            print(the_list_.shape)\n",
    "            \n",
    "            angle3_, p1_tf_stacked_full_, labels_ = sess.partial_run(h, [angle3, \n",
    "                                                                         p1_tf_stacked_full, labels], \n",
    "                                                                         feed_dict={the_list: the_list_})\n",
    "            \n",
    "            print(np.array(angle3_).shape)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "#         print(num_examples)\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_examples = 0\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(tf.local_variables_initializer())\n",
    "#     coord = tf.train.Coordinator()  \n",
    "#     threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "# #     batch = tf.train.shuffle_batch([id_], 10, 200, 100)\n",
    "#     try:\n",
    "#         step = 0\n",
    "#         while not coord.should_stop():\n",
    "#             start_time = time.time()\n",
    "# #             ids_, data_, labels_, length_ = sess.run([ids, data, labels, length])\n",
    "            \n",
    "# #             out = sess.run(outputs)\n",
    "#             test, length_, outputs_, outputs_conc_, pred_ = sess.run([tile_test, length, outputs, outputs_conc, pred])\n",
    "# #             print(outputs_[0].shape, outputs_[1].shape, outputs_conc_.shape, pred_.shape, loss)\n",
    "            \n",
    "#             labels_ = sess.run([labels])\n",
    "            \n",
    "#             print(np.array(labels_).shape, np.array(labels_)[0][:2][:9])\n",
    "#             test = np.array(labels_)[0][:2]\n",
    "# #             print(ids_.shape, data_.shape, length_)\n",
    "# #             print \"grabbing\"\n",
    "# #             e, l = sess.run([example_batch, label_batch])\n",
    "# #             num_examples = num_examples + ids_.shape[0]\n",
    "# #             print \"num_examples = \" + str(num_examples)\n",
    "#             duration = time.time() - start_time\n",
    "\n",
    "#     except tf.errors.OutOfRangeError:\n",
    "#         print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "# #         print(num_examples)\n",
    "#     finally:\n",
    "#         # When done, ask the threads to stop.\n",
    "#         coord.request_stop()\n",
    "\n",
    "#         # Wait for threads to finish.\n",
    "#         coord.join(threads)\n",
    "#         sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
