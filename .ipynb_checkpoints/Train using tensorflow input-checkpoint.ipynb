{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "NUM_AAS = 20\n",
    "NUM_DIMENSIONS = 3\n",
    "\n",
    "def masking_matrix(mask, name=None):\n",
    "    \"\"\" Constructs a masking matrix to zero out pairwise distances due to missing residues or padding. \n",
    "\n",
    "    Args:\n",
    "        mask: 0/1 vector indicating whether a position should be masked (0) or not (1)\n",
    "\n",
    "    Returns:\n",
    "        A square matrix with all 1s except for rows and cols whose corresponding indices in mask are set to 0.\n",
    "        [MAX_SEQ_LENGTH, MAX_SEQ_LENGTH]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'masking_matrix', [mask]) as scope:\n",
    "        mask = tf.convert_to_tensor(mask, name='mask')\n",
    "\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        base = tf.ones([tf.size(mask), tf.size(mask)])\n",
    "        matrix_mask = base * mask * tf.transpose(mask)\n",
    "\n",
    "        return matrix_mask\n",
    "        \n",
    "def read_protein(filename_queue, max_length, num_evo_entries=21, name=None):\n",
    "    \"\"\" Reads and parses a ProteinNet TF Record. \n",
    "\n",
    "        Primary sequences are mapped onto 20-dimensional one-hot vectors.\n",
    "        Evolutionary sequences are mapped onto num_evo_entries-dimensional real-valued vectors.\n",
    "        Secondary structures are mapped onto ints indicating one of 8 class labels.\n",
    "        Tertiary coordinates are flattened so that there are 3 times as many coordinates as \n",
    "        residues.\n",
    "\n",
    "        Evolutionary, secondary, and tertiary entries are optional.\n",
    "\n",
    "    Args:\n",
    "        filename_queue: TF queue for reading files\n",
    "        max_length:     Maximum length of sequence (number of residues) [MAX_LENGTH]. Not a \n",
    "                        TF tensor and is thus a fixed value.\n",
    "\n",
    "    Returns:\n",
    "        id: string identifier of record\n",
    "        one_hot_primary: AA sequence as one-hot vectors\n",
    "        evolutionary: PSSM sequence as vectors\n",
    "        secondary: DSSP sequence as int class labels\n",
    "        tertiary: 3D coordinates of structure\n",
    "        matrix_mask: Masking matrix to zero out pairwise distances in the masked regions\n",
    "        pri_length: Length of amino acid sequence\n",
    "        keep: True if primary length is less than or equal to max_length\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'read_protein', []) as scope:\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "        context, features = tf.parse_single_sequence_example(serialized_example,\n",
    "                                context_features={'id': tf.FixedLenFeature((1,), tf.string)},\n",
    "                                sequence_features={\n",
    "                                    'primary':      tf.FixedLenSequenceFeature((1,),               tf.int64),\n",
    "                                    'evolutionary': tf.FixedLenSequenceFeature((num_evo_entries,), tf.float32, allow_missing=True),\n",
    "                                    'secondary':    tf.FixedLenSequenceFeature((1,),               tf.int64,   allow_missing=True),\n",
    "                                    'tertiary':     tf.FixedLenSequenceFeature((NUM_DIMENSIONS,),  tf.float32, allow_missing=True),\n",
    "                                    'mask':         tf.FixedLenSequenceFeature((1,),               tf.float32, allow_missing=True)})\n",
    "        id_ = context['id'][0]\n",
    "        primary =   tf.to_int32(features['primary'][:, 0])\n",
    "        evolutionary =          features['evolutionary']\n",
    "        secondary = tf.to_int32(features['secondary'][:, 0])\n",
    "        tertiary =              features['tertiary']\n",
    "        mask =                  features['mask'][:, 0]\n",
    "\n",
    "        pri_length = tf.size(primary)\n",
    "        keep = pri_length <= max_length\n",
    "\n",
    "        one_hot_primary = tf.one_hot(primary, NUM_AAS)\n",
    "\n",
    "        # Generate tertiary masking matrix--if mask is missing then assume all residues are present\n",
    "        mask = tf.cond(tf.not_equal(tf.size(mask), 0), lambda: mask, lambda: tf.ones([pri_length]))\n",
    "        ter_mask = masking_matrix(mask, name='ter_mask')        \n",
    "\n",
    "        return id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\1', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\10', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\11', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\12', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\13', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\14', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\15', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\16', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\17', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\18', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\19', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\2', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\20', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\21', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\22', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\23', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\24', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\25', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\26', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\27', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\28', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\29', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\3', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\30', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\31', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\32', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\33', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\34', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\35', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\36', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\37', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\38', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\39', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\4', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\40', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\41', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\5', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\6', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\7', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\8', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\9']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) (32, 389, 20)\n",
      "(32,) (32, 858, 20)\n",
      "(32,) (32, 541, 20)\n",
      "(32,) (32, 608, 20)\n",
      "(32,) (32, 495, 20)\n",
      "(32,) (32, 511, 20)\n",
      "(32,) (32, 520, 20)\n",
      "(32,) (32, 816, 20)\n",
      "(32,) (32, 389, 20)\n",
      "(32,) (32, 858, 20)\n",
      "(32,) (32, 541, 20)\n",
      "(32,) (32, 608, 20)\n",
      "(32,) (32, 495, 20)\n",
      "(32,) (32, 511, 20)\n",
      "(32,) (32, 520, 20)\n",
      "(32,) (32, 816, 20)\n",
      "Done training for 2 epochs, 0 steps.\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_epochs = 2\n",
    "a_path = r'C:\\Users\\Michal\\Desktop\\ITU NLP\\casp7\\training\\30\\*'\n",
    "# init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())\n",
    "base_names = glob.glob(a_path)\n",
    "base_tensor = tf.convert_to_tensor(base_names[:1])\n",
    "file_queue = tf.train.string_input_producer(\n",
    "    base_tensor,\n",
    "    num_epochs=num_epochs,\n",
    "    shuffle=False # Note: must set shuffle to False\n",
    ")\n",
    "\n",
    "res = read_protein(file_queue, max_length=100)\n",
    "id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep = res\n",
    "# one_hot_primary = tf.slice(one_hot_primary, 0, 10)\n",
    "\n",
    "batch_size=32\n",
    "capacity=1000\n",
    "min_after_dequeue=100\n",
    "# ids, data, length = tf.train.shuffle_batch(\n",
    "#       [id_, one_hot_primary, pri_length], \n",
    "#       batch_size=batch_size, \n",
    "#       capacity=capacity,\n",
    "#       min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "ids, data, length = tf.train.batch(\n",
    "      [id_, one_hot_primary, pri_length], \n",
    "      batch_size=batch_size, \n",
    "      capacity=capacity, dynamic_pad=True)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "num_examples = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "#     batch = tf.train.shuffle_batch([id_], 10, 200, 100)\n",
    "    try:\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            start_time = time.time()\n",
    "            ids_, data_, length_ = sess.run([ids, data, length])\n",
    "            print(ids_.shape, data_.shape)\n",
    "#             print \"grabbing\"\n",
    "#             e, l = sess.run([example_batch, label_batch])\n",
    "            num_examples = num_examples + e.shape[0]\n",
    "#             print \"num_examples = \" + str(num_examples)\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "        print(num_examples)\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
