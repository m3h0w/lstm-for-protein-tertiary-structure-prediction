{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you extracted following files into the same directory as this notebook:\n",
    "- training_50_dih.joblib\n",
    "- validation_dih.joblib\n",
    "- le.joblib\n",
    "- ohe.joblib\n",
    "\n",
    "And run 'pip install joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dihedral(p):\n",
    "    \"\"\"Praxeolitic formula\n",
    "    1 sqrt, 1 cross product\"\"\"\n",
    "    p0 = p[0]\n",
    "    p1 = p[1]\n",
    "    p2 = p[2]\n",
    "    p3 = p[3]\n",
    "\n",
    "    b0 = -1.0*(p1 - p0)\n",
    "    b1 = p2 - p1\n",
    "    b2 = p3 - p2\n",
    "\n",
    "    # normalize b1 so that it does not influence magnitude of vector\n",
    "    # rejections that come next\n",
    "    b1 /= np.linalg.norm(b1)\n",
    "\n",
    "    # vector rejections\n",
    "    # v = projection of b0 onto plane perpendicular to b1\n",
    "    #   = b0 minus component that aligns with b1\n",
    "    # w = projection of b2 onto plane perpendicular to b1\n",
    "    #   = b2 minus component that aligns with b1\n",
    "    v = b0 - np.dot(b0, b1)*b1\n",
    "    w = b2 - np.dot(b2, b1)*b1\n",
    "\n",
    "    # angle between v and w in a plane is the torsion angle\n",
    "    # v and w may not be normalized but that's fine since tan is y/x\n",
    "    x = np.dot(v, w)\n",
    "    y = np.dot(np.cross(b1, v), w)\n",
    "    return np.arctan2(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_order = [\"[ID]\", \"[PRIMARY]\", \"[EVOLUTIONARY]\", \"[TERTIARY]\", \"[MASK]\"]\n",
    "\n",
    "# change this path to match the path of where you have the\n",
    "# training_50 and validation files\n",
    "txt_data_path = '/home/mikey/Data/ProteinNet/casp7_txt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def count_protein(raw_txt_data):\n",
    "    data = filter_line_end(raw_txt_data)\n",
    "    result = 0\n",
    "    for line in data:\n",
    "        if line == data_order[0]: #id\n",
    "            result += 1\n",
    "    return result\n",
    "\n",
    "def filter_line_end(data):\n",
    "    return [str_.replace('\\n', '') for str_ in data]\n",
    "\n",
    "def get_mask_from_all_data(data, lim):\n",
    "    result = []\n",
    "    protein_count = 0\n",
    "    flag = False\n",
    "    for line in data:\n",
    "        if line == data_order[0]: #id\n",
    "            flag = False\n",
    "        if flag:\n",
    "            result.append(line)\n",
    "        if line == data_order[4]: #mask\n",
    "            flag = True\n",
    "        if line == data_order[0]: #id\n",
    "            protein_count += 1\n",
    "        if lim and protein_count > lim:\n",
    "            return result\n",
    "    return result\n",
    "\n",
    "def get_primary_from_all_data(data, lim):\n",
    "    result = []\n",
    "    protein_count = 0\n",
    "    flag = False\n",
    "    for line in data:\n",
    "        if line == data_order[2]: #evolutionary\n",
    "            flag = False\n",
    "        if flag:\n",
    "            result.append(line)\n",
    "        if line == data_order[1]: #primary\n",
    "            flag = True\n",
    "        if line == data_order[0]: #id\n",
    "            protein_count += 1\n",
    "        if lim and protein_count > lim:\n",
    "            return result\n",
    "    return result\n",
    "\n",
    "def get_evolutionary_from_all_data(data, lim):\n",
    "    result = []\n",
    "    protein_count = 0\n",
    "    flag = False\n",
    "    for line in data:\n",
    "        if line == data_order[-2]: #mask\n",
    "            flag = False\n",
    "        if flag:\n",
    "            result.append(line)\n",
    "        if line == data_order[-3]: #tertiary\n",
    "            flag = True\n",
    "        if line == data_order[0]: #id\n",
    "            protein_count += 1\n",
    "        if lim and protein_count > lim:\n",
    "            return result\n",
    "    return result\n",
    "\n",
    "def get_tertiary_from_all_data(data, lim):\n",
    "    result = []\n",
    "    protein_count = 0\n",
    "    flag = False\n",
    "    for line in data:\n",
    "        if line == data_order[-1]: #mask\n",
    "            flag = False\n",
    "        if flag:\n",
    "            result.append(line)\n",
    "        if line == data_order[-2]: #tertiary\n",
    "            flag = True\n",
    "        if line == data_order[0]: #id\n",
    "            protein_count += 1\n",
    "        if lim and protein_count > lim:\n",
    "            return result\n",
    "    return result\n",
    "\n",
    "def group_aminoacids_together(data, every_n):\n",
    "    data_expanded = [np.asarray(np.expand_dims(t.split('\\t'), 1), dtype=np.float32) for t in data]\n",
    "    result = []\n",
    "    for i in tqdm(range(0,len(data_expanded),every_n)):\n",
    "        # group together every_n entries (e.g. 3 for tertiary and 21 for evo)\n",
    "        result.append(np.concatenate([data_expanded[i+r] for r in range(every_n)], axis=1))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def parse_tertiary_from_file(path, data_lim=None):\n",
    "    with open(path) as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    data_ = filter_line_end(data[:data_lim])        \n",
    "    only_tertiary = get_tertiary_from_all_data(data_, data_lim)\n",
    "    return group_aminoacids_together(only_tertiary, every_n=3)\n",
    "\n",
    "def parse_evolutionary_from_file(path, data_lim=None):\n",
    "    with open(txt_data_path + file_name) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    data_ = filter_line_end(data)\n",
    "    print(\"Loaded data and filtered line endings\")\n",
    "    only_evo = get_evolutionary_from_all_data(data_, data_lim)\n",
    "    print(\"Extracted evolutionary data\")\n",
    "    res = group_aminoacids_together(only_evo, every_n = 21)\n",
    "    print(\"Grouped 21's together\")\n",
    "    return res\n",
    "\n",
    "def parse_primary_from_file(path, data_lim=None):\n",
    "    with open(txt_data_path + file_name) as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    data_ = filter_line_end(data)\n",
    "    print(\"Loaded data and filtered line endings\")\n",
    "    primary = get_primary_from_all_data(data_, data_lim)\n",
    "    print(\"Extracted primary data\")\n",
    "    le = load_file('le.joblib')\n",
    "    ohe = load_file('ohe.joblib')\n",
    "    primary_in_floats = [le.transform([_ for _ in c]) for c in primary]\n",
    "    primary_encoded = [ohe.transform(a.reshape(-1,1)).toarray() for a in primary_in_floats]\n",
    "    print(\"Encoded primary sequences\")\n",
    "    return primary_encoded\n",
    "\n",
    "def parse_mask_from_file(path, data_lim=None):\n",
    "    with open(txt_data_path + file_name) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    data_ = filter_line_end(data)\n",
    "    print(\"Loaded data and filtered line endings\")\n",
    "    only_mask = get_mask_from_all_data(data_, data_lim)\n",
    "    only_mask = [mask for mask in only_mask if mask != '']\n",
    "    only_mask_ = []\n",
    "    for mask in only_mask:\n",
    "        only_mask_.append(np.array(list(mask)) == '+')\n",
    "\n",
    "    print(\"Extracted mask data\")\n",
    "    return only_mask_\n",
    "\n",
    "def get_dih(protein_tertiary):\n",
    "    p = protein_tertiary\n",
    "    r = p.shape[0]\n",
    "    a_list = list(range(r))\n",
    "    the_list = np.array([a_list[slice(i, i+4)] for i in range(r - 4+1)])\n",
    "    slices = np.asarray(p[the_list], dtype=np.float32)\n",
    "    one_dih = np.array([dihedral(slice_) for slice_ in slices])\n",
    "    one_dih = np.insert(one_dih, 0, None)\n",
    "    one_dih = np.append(one_dih, [None,None])\n",
    "    return one_dih.reshape(-1,3)\n",
    "\n",
    "def save_file(data, path):\n",
    "    joblib.dump(data, path) \n",
    "    \n",
    "def load_file(path):\n",
    "    return joblib.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was used to generate the dihedral angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'training_50'\n",
    "# # file_name = 'validation'\n",
    "# tertiary = parse_tertiary_from_file(txt_data_path + file_name)\n",
    "\n",
    "# dih = []\n",
    "# for protein in tqdm(tertiary):\n",
    "#     dih.append(get_dih(protein))\n",
    "# save_file(dih, file_name + '_dih.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_lim is important because so far the full 13000 proteins crashes my system with 16Gb of ram\n",
    "\n",
    "Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and filtered line endings\n",
      "Extracted primary data\n",
      "Encoded primary sequences\n",
      "Loaded data and filtered line endings\n",
      "Extracted evolutionary data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 75934.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped 21's together\n",
      "Loaded data and filtered line endings\n",
      "Extracted mask data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, (70, 20), 8000, (70, 21), 8000, (70, 3), 8000, (70,))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'training_50'\n",
    "data_lim = 8000\n",
    "prim_train, evo_train, dih_train, mask_train = (parse_primary_from_file(txt_data_path + file_name, data_lim), \n",
    "                                    parse_evolutionary_from_file(txt_data_path + file_name, data_lim),\n",
    "                                    load_file('./'+file_name+'_dih.joblib'),\n",
    "                                               parse_mask_from_file(txt_data_path + file_name, data_lim))\n",
    "dih_train = dih_train[:data_lim]\n",
    "len(prim_train), prim_train[0].shape, len(evo_train), evo_train[0].shape, len(dih_train), dih_train[0].shape, len(mask_train), mask_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.4'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and filtered line endings\n",
      "Extracted primary data\n",
      "Encoded primary sequences\n",
      "Loaded data and filtered line endings\n",
      "Extracted evolutionary data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224/224 [00:00<00:00, 57568.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped 21's together\n",
      "Loaded data and filtered line endings\n",
      "Extracted mask data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(224, (269, 20), 224, (269, 21), 224, (269, 3), 224, (269,))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'validation'\n",
    "prim_valid, evo_valid, dih_valid, mask_valid = (parse_primary_from_file(txt_data_path + file_name), \n",
    "                                    parse_evolutionary_from_file(txt_data_path + file_name),\n",
    "                                    load_file('./'+ file_name + '_dih.joblib'),\n",
    "                                               parse_mask_from_file(txt_data_path + file_name))\n",
    "len(prim_valid), prim_valid[0].shape, len(evo_valid), evo_valid[0].shape, len(dih_valid), dih_valid[0].shape, len(mask_valid), mask_valid[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the data and limit protein length. Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded\n",
      "(7623,) (70, 20)\n",
      "padded\n",
      "(7623,) (70, 21)\n",
      "padded\n",
      "(7623,) (70, 3)\n",
      "padded\n",
      "(7623,) (70, 1)\n",
      "padded\n",
      "(213,) (269, 20)\n",
      "padded\n",
      "(213,) (269, 21)\n",
      "padded\n",
      "(213,) (269, 3)\n",
      "padded\n",
      "(213,) (269, 1)\n"
     ]
    }
   ],
   "source": [
    "def fix_nans(a):\n",
    "    where_are_NaNs = np.isnan(a)\n",
    "    a[where_are_NaNs] = 0\n",
    "    return a\n",
    "\n",
    "def pad_array(array, max_len, value=0., dtype=np.float32):\n",
    "    print(\"padded\")\n",
    "    if(len(array[0].shape) < 2):\n",
    "        array = np.array([a.reshape(-1,1) for a in array])\n",
    "    print(array.shape, array[0].shape)\n",
    "    return np.asarray([np.pad(a, [(max_len - len(a), 0), (0,0)], mode='constant', constant_values=value) for a in array], dtype=dtype)\n",
    "\n",
    "def limit_length_and_pad(prim, evo, dih, mask, max_length):\n",
    "    len_mask = np.array([len(el) for el in prim]) <= max_length\n",
    "    prim_lim, evo_lim, dih_lim, mask_lim = (np.array(prim)[len_mask], np.array(evo)[len_mask], \n",
    "                                            np.array(dih)[len_mask], np.array(mask)[len_mask])\n",
    "    prim_pad, evo_pad, dih_pad, mask_pad = (pad_array(prim_lim, max_length), pad_array(evo_lim, max_length), \n",
    "                                            pad_array(dih_lim, max_length), pad_array(mask_lim, max_length, value=False, dtype=np.bool))\n",
    "    mask_pad = mask_pad.reshape(mask_pad.shape[0], -1) # this is necessary because numpy expects this shape to use mask as an index\n",
    "    return fix_nans(prim_pad), fix_nans(evo_pad), fix_nans(dih_pad), fix_nans(mask_pad)\n",
    "\n",
    "max_len = 500\n",
    "prim_, evo_, dih_, mask_ = limit_length_and_pad(prim_train, evo_train, dih_train, mask_train, max_len)\n",
    "x_train, y_train = np.concatenate([prim_, evo_], axis=2), dih_\n",
    "prim_v, evo_v, dih_v, mask_v = limit_length_and_pad(prim_valid, evo_valid, dih_valid, mask_valid, max_len)\n",
    "x_valid, y_valid = np.concatenate([prim_v, evo_v], axis=2), dih_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7623, 500, 41)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[np.logical_not(mask_)] = -1.\n",
    "x_valid[np.logical_not(mask_v)] = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a simple regression task to check if it trains on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7623 samples, validate on 213 samples\n",
      "Epoch 1/10\n",
      " 900/7623 [==>...........................] - ETA: 40s - loss: 0.6327 - mean_absolute_error: 0.6327"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-3f26506e6300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m               metrics=['mean_absolute_error'])\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# score = model.evaluate(x_valid, y_valid, batch_size=16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Lambda, Masking, concatenate\n",
    "from keras.layers import LSTM, Conv1D, Input\n",
    "\n",
    "inputs = Input(shape=(max_len,41))\n",
    "x1 = Masking(mask_value=-1.)(inputs)\n",
    "x2 = LSTM(12, return_sequences=True)(x1)\n",
    "# x12 = concatenate([x1,x2])\n",
    "# x3 = LSTM(64, return_sequences=True)(x2)\n",
    "# x23 = concatenate([x2,x3])\n",
    "x4 = Dense(3, activation='tanh')(x2)\n",
    "y = Lambda(lambda x: x*np.pi)(x4)\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Masking(mask_value=-1.))\n",
    "# model.add(LSTM(128, return_sequences=True))\n",
    "# # model.add(LSTM(128, return_sequences=True))\n",
    "# # model.add(Conv1D(32, 15, padding='same'))\n",
    "# # model.add(Conv1D(64, 15, padding='same'))\n",
    "# # model.add(Conv1D(128, 15, padding='same'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(3, activation='tanh'))\n",
    "# model.add(Lambda(lambda x: x*np.pi))\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_valid, y_valid), batch_size=100, epochs=10)\n",
    "histories.append(history)\n",
    "# score = model.evaluate(x_valid, y_valid, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d_1 (None, 200, 41) (None, 200, 32)\n",
      "conv1d_2 (None, 200, 32) (None, 200, 64)\n",
      "conv1d_3 (None, 200, 64) (None, 200, 128)\n",
      "dense_1 (None, 200, 128) (None, 200, 3)\n",
      "lambda_1 (None, 200, 3) (None, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.input_shape, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atan2(x, y, epsilon=1.0e-12):\n",
    "    x = tf.where(tf.equal(x, 0.0), x+epsilon, x)\n",
    "    y = tf.where(tf.equal(y, 0.0), y+epsilon, y)    \n",
    "    angle = tf.where(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))\n",
    "    angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\n",
    "    angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\n",
    "    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\n",
    "    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\n",
    "    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), tf.zeros_like(x), angle)\n",
    "    return angle\n",
    "\n",
    "# y in radians\n",
    "def rmse_360_2(y_true, y_pred):\n",
    "    y_pred = tf.reshape(y_pred, shape=(tf.shape(y_pred)[0], tf.shape(y_pred)[1], 3, 2))\n",
    "    y1 = atan2(y_pred[:,:,0,0], y_pred[:,:,0,1]), atan2(y_pred[:,:,1,0], y_pred[:,:,1,1]), atan2(y_pred[:,:,2,0], y_pred[:,:,2,1]) \n",
    "    y1 = tf.transpose(y1, perm=[1,2,0])\n",
    "    return K.mean(K.abs(y1 - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78539816 -0.78539816]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    print(atan2(np.array([1.,1.]),np.array([1., -1])).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3053 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "3053/3053 [==============================] - 20s 6ms/step - loss: 1.5151 - val_loss: 1.3630\n",
      "Epoch 2/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 1.3592 - val_loss: 1.2831\n",
      "Epoch 3/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 1.2825 - val_loss: 1.1911\n",
      "Epoch 4/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 1.2089 - val_loss: 1.1405\n",
      "Epoch 5/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 1.1732 - val_loss: 1.1053\n",
      "Epoch 6/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 1.1393 - val_loss: 1.0797\n",
      "Epoch 7/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 1.1142 - val_loss: 1.0595\n",
      "Epoch 8/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 1.0992 - val_loss: 1.0414\n",
      "Epoch 9/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 1.0765 - val_loss: 1.0049\n",
      "Epoch 10/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 1.0375 - val_loss: 0.9656\n",
      "Epoch 11/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 1.0002 - val_loss: 0.9319\n",
      "Epoch 12/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9714 - val_loss: 0.9019\n",
      "Epoch 13/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9551 - val_loss: 0.8957\n",
      "Epoch 14/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9481 - val_loss: 0.8848\n",
      "Epoch 15/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9411 - val_loss: 0.8838\n",
      "Epoch 16/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9448 - val_loss: 0.8795\n",
      "Epoch 17/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9359 - val_loss: 0.8756\n",
      "Epoch 18/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9362 - val_loss: 0.8839\n",
      "Epoch 19/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9415 - val_loss: 0.8733\n",
      "Epoch 20/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9281 - val_loss: 0.8680\n",
      "Epoch 21/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9330 - val_loss: 0.8809\n",
      "Epoch 22/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9361 - val_loss: 0.8697\n",
      "Epoch 23/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9259 - val_loss: 0.8664\n",
      "Epoch 24/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9280 - val_loss: 0.8671\n",
      "Epoch 25/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9302 - val_loss: 0.8699\n",
      "Epoch 26/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9259 - val_loss: 0.8605\n",
      "Epoch 27/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9214 - val_loss: 0.8629\n",
      "Epoch 28/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9283 - val_loss: 0.8725\n",
      "Epoch 29/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9224 - val_loss: 0.8557\n",
      "Epoch 30/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9183 - val_loss: 0.8657\n",
      "Epoch 31/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9259 - val_loss: 0.8584\n",
      "Epoch 32/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9137 - val_loss: 0.8539\n",
      "Epoch 33/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9229 - val_loss: 0.8698\n",
      "Epoch 34/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9170 - val_loss: 0.8496\n",
      "Epoch 35/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9097 - val_loss: 0.8578\n",
      "Epoch 36/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9201 - val_loss: 0.8567\n",
      "Epoch 37/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9086 - val_loss: 0.8458\n",
      "Epoch 38/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9139 - val_loss: 0.8628\n",
      "Epoch 39/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9105 - val_loss: 0.8458\n",
      "Epoch 40/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9081 - val_loss: 0.8595\n",
      "Epoch 41/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9168 - val_loss: 0.8517\n",
      "Epoch 42/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9030 - val_loss: 0.8479\n",
      "Epoch 43/50\n",
      "3053/3053 [==============================] - 16s 5ms/step - loss: 0.9131 - val_loss: 0.8539\n",
      "Epoch 44/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9049 - val_loss: 0.8418\n",
      "Epoch 45/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9069 - val_loss: 0.8566\n",
      "Epoch 46/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9040 - val_loss: 0.8401\n",
      "Epoch 47/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9055 - val_loss: 0.8574\n",
      "Epoch 48/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9035 - val_loss: 0.8387\n",
      "Epoch 49/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9037 - val_loss: 0.8578\n",
      "Epoch 50/50\n",
      "3053/3053 [==============================] - 17s 5ms/step - loss: 0.9056 - val_loss: 0.8385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa082dee908>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Lambda\n",
    "from keras.layers import LSTM, Conv1D\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Conv1D(32, 5, padding='same'))\n",
    "model.add(Conv1D(64, 10, padding='same'))\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "# model.add(Conv1D(128, 15, padding='same'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(6))\n",
    "# model.add(Lambda(lambda x: x*np.pi))\n",
    "\n",
    "optm = Adam(lr=0.00001)\n",
    "model.compile(loss=rmse_360_2,\n",
    "              optimizer=optm)\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), batch_size=32, epochs=50)\n",
    "# score = model.evaluate(x_valid, y_valid, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
