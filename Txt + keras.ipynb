{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you extracted following files into the same directory as this notebook:\n",
    "- training_50_dih.joblib\n",
    "- validation_dih.joblib\n",
    "- le.joblib\n",
    "- ohe.joblib\n",
    "\n",
    "And run 'pip install joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import data_loader as dl\n",
    "import data_transformer as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this path to match the path of where you have the\n",
    "# training_50 and validation files\n",
    "txt_data_path = '/home/mikey/Data/ProteinNet/casp7_txt/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_lim is important because so far the full 13000 proteins crashes my system with 16Gb of ram\n",
    "\n",
    "Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and filtered line endings\n",
      "Extracted primary data\n",
      "Encoded primary sequences\n",
      "Loaded data and filtered line endings\n",
      "Extracted evolutionary data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 74953.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped 21's together\n",
      "Loaded data and filtered line endings\n",
      "Extracted mask data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, (70, 20), 8000, (70, 21), 13024, (70, 3), 8000, (70,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'training_50'\n",
    "data_lim = 8000\n",
    "prim_train, evo_train, dih_train, mask_train = (dl.parse_primary_from_file(txt_data_path + file_name, data_lim), \n",
    "                                                dl.parse_evolutionary_from_file(txt_data_path + file_name, data_lim),\n",
    "                                                dl.load_file('./'+file_name+'_dih.joblib'),\n",
    "                                                dl.parse_mask_from_file(txt_data_path + file_name, data_lim))\n",
    "dih_train = dih_train[:data_lim]\n",
    "len(prim_train), prim_train[0].shape, len(evo_train), evo_train[0].shape, len(dih_train), dih_train[0].shape, len(mask_train), mask_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and filtered line endings\n",
      "Extracted primary data\n",
      "Encoded primary sequences\n",
      "Loaded data and filtered line endings\n",
      "Extracted evolutionary data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224/224 [00:00<00:00, 54496.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped 21's together\n",
      "Loaded data and filtered line endings\n",
      "Extracted mask data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(224, (269, 20), 224, (269, 21), 224, (269, 3), 224, (269,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'validation'\n",
    "prim_valid, evo_valid, dih_valid, mask_valid = (dl.parse_primary_from_file(txt_data_path + file_name), \n",
    "                                                dl.parse_evolutionary_from_file(txt_data_path + file_name),\n",
    "                                                dl.load_file('./'+ file_name + '_dih.joblib'),\n",
    "                                                dl.parse_mask_from_file(txt_data_path + file_name))\n",
    "len(prim_valid), prim_valid[0].shape, len(evo_valid), evo_valid[0].shape, len(dih_valid), dih_valid[0].shape, len(mask_valid), mask_valid[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the data and limit protein length. Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded\n",
      "(7623,) (70, 20)\n",
      "padded\n",
      "(7623,) (70, 21)\n",
      "padded\n",
      "(7623,) (70, 3)\n",
      "padded\n",
      "(7623,) (70, 1)\n",
      "padded\n",
      "(213,) (269, 20)\n",
      "padded\n",
      "(213,) (269, 21)\n",
      "padded\n",
      "(213,) (269, 3)\n",
      "padded\n",
      "(213,) (269, 1)\n"
     ]
    }
   ],
   "source": [
    "max_len = 500\n",
    "prim_, evo_, dih_, mask_ = dt.limit_length_and_pad(prim_train, evo_train, dih_train, mask_train, max_len)\n",
    "x_train, y_train = np.concatenate([prim_, evo_], axis=2), dih_\n",
    "prim_v, evo_v, dih_v, mask_v = dt.limit_length_and_pad(prim_valid, evo_valid, dih_valid, mask_valid, max_len)\n",
    "x_valid, y_valid = np.concatenate([prim_v, evo_v], axis=2), dih_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7623, 500, 41)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[np.logical_not(mask_)] = -1.\n",
    "x_valid[np.logical_not(mask_v)] = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a simple regression task to check if it trains on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7623 samples, validate on 213 samples\n",
      "Epoch 1/10\n",
      "7623/7623 [==============================] - 82s 11ms/step - loss: 0.5948 - mean_absolute_error: 0.5948 - val_loss: 0.5815 - val_mean_absolute_error: 0.5815\n",
      "Epoch 2/10\n",
      "7623/7623 [==============================] - 81s 11ms/step - loss: 0.5593 - mean_absolute_error: 0.5593 - val_loss: 0.5682 - val_mean_absolute_error: 0.5682\n",
      "Epoch 3/10\n",
      "7623/7623 [==============================] - 81s 11ms/step - loss: 0.5508 - mean_absolute_error: 0.5508 - val_loss: 0.5614 - val_mean_absolute_error: 0.5614\n",
      "Epoch 4/10\n",
      "7623/7623 [==============================] - 82s 11ms/step - loss: 0.5450 - mean_absolute_error: 0.5450 - val_loss: 0.5558 - val_mean_absolute_error: 0.5558\n",
      "Epoch 5/10\n",
      "7623/7623 [==============================] - 82s 11ms/step - loss: 0.5401 - mean_absolute_error: 0.5401 - val_loss: 0.5513 - val_mean_absolute_error: 0.5513\n",
      "Epoch 6/10\n",
      "7623/7623 [==============================] - 82s 11ms/step - loss: 0.5364 - mean_absolute_error: 0.5364 - val_loss: 0.5479 - val_mean_absolute_error: 0.5479\n",
      "Epoch 7/10\n",
      "7623/7623 [==============================] - 82s 11ms/step - loss: 0.5334 - mean_absolute_error: 0.5334 - val_loss: 0.5452 - val_mean_absolute_error: 0.5452\n",
      "Epoch 8/10\n",
      "2300/7623 [========>.....................] - ETA: 55s - loss: 0.5294 - mean_absolute_error: 0.5294"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Lambda, Masking, concatenate\n",
    "from keras.layers import LSTM, Conv1D, Input\n",
    "\n",
    "inputs = Input(shape=(max_len,41))\n",
    "x1 = Masking(mask_value=-1.)(inputs)\n",
    "x2 = LSTM(10, return_sequences=True)(x1)\n",
    "x12 = concatenate([x1,x2])\n",
    "x3 = LSTM(10, return_sequences=True)(x12)\n",
    "x4 = Dense(3, activation='tanh')(x3)\n",
    "y = Lambda(lambda x: x*np.pi)(x4)\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Masking(mask_value=-1.))\n",
    "# model.add(LSTM(128, return_sequences=True))\n",
    "# # model.add(LSTM(128, return_sequences=True))\n",
    "# # model.add(Conv1D(32, 15, padding='same'))\n",
    "# # model.add(Conv1D(64, 15, padding='same'))\n",
    "# # model.add(Conv1D(128, 15, padding='same'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(3, activation='tanh'))\n",
    "# model.add(Lambda(lambda x: x*np.pi))\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_valid, y_valid), batch_size=100, epochs=10)\n",
    "histories.append(history)\n",
    "# score = model.evaluate(x_valid, y_valid, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
