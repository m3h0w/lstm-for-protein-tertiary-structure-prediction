{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "NUM_AAS = 20\n",
    "NUM_DIMENSIONS = 3\n",
    "\n",
    "def masking_matrix(mask, name=None):\n",
    "    \"\"\" Constructs a masking matrix to zero out pairwise distances due to missing residues or padding. \n",
    "\n",
    "    Args:\n",
    "        mask: 0/1 vector indicating whether a position should be masked (0) or not (1)\n",
    "\n",
    "    Returns:\n",
    "        A square matrix with all 1s except for rows and cols whose corresponding indices in mask are set to 0.\n",
    "        [MAX_SEQ_LENGTH, MAX_SEQ_LENGTH]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'masking_matrix', [mask]) as scope:\n",
    "        mask = tf.convert_to_tensor(mask, name='mask')\n",
    "\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        base = tf.ones([tf.size(mask), tf.size(mask)])\n",
    "        matrix_mask = base * mask * tf.transpose(mask)\n",
    "\n",
    "        return matrix_mask\n",
    "        \n",
    "def read_protein(filename_queue, max_length, num_evo_entries=21, name=None):\n",
    "    \"\"\" Reads and parses a ProteinNet TF Record. \n",
    "\n",
    "        Primary sequences are mapped onto 20-dimensional one-hot vectors.\n",
    "        Evolutionary sequences are mapped onto num_evo_entries-dimensional real-valued vectors.\n",
    "        Secondary structures are mapped onto ints indicating one of 8 class labels.\n",
    "        Tertiary coordinates are flattened so that there are 3 times as many coordinates as \n",
    "        residues.\n",
    "\n",
    "        Evolutionary, secondary, and tertiary entries are optional.\n",
    "\n",
    "    Args:\n",
    "        filename_queue: TF queue for reading files\n",
    "        max_length:     Maximum length of sequence (number of residues) [MAX_LENGTH]. Not a \n",
    "                        TF tensor and is thus a fixed value.\n",
    "\n",
    "    Returns:\n",
    "        id: string identifier of record\n",
    "        one_hot_primary: AA sequence as one-hot vectors\n",
    "        evolutionary: PSSM sequence as vectors\n",
    "        secondary: DSSP sequence as int class labels\n",
    "        tertiary: 3D coordinates of structure\n",
    "        matrix_mask: Masking matrix to zero out pairwise distances in the masked regions\n",
    "        pri_length: Length of amino acid sequence\n",
    "        keep: True if primary length is less than or equal to max_length\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'read_protein', []) as scope:\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "        context, features = tf.parse_single_sequence_example(serialized_example,\n",
    "                                context_features={'id': tf.FixedLenFeature((1,), tf.string)},\n",
    "                                sequence_features={\n",
    "                                    'primary':      tf.FixedLenSequenceFeature((1,),               tf.int64),\n",
    "                                    'evolutionary': tf.FixedLenSequenceFeature((num_evo_entries,), tf.float32, allow_missing=True),\n",
    "                                    'secondary':    tf.FixedLenSequenceFeature((1,),               tf.int64,   allow_missing=True),\n",
    "                                    'tertiary':     tf.FixedLenSequenceFeature((NUM_DIMENSIONS,),  tf.float32, allow_missing=True),\n",
    "                                    'mask':         tf.FixedLenSequenceFeature((1,),               tf.float32, allow_missing=True)})\n",
    "        id_ = context['id'][0]\n",
    "        primary =   tf.to_int32(features['primary'][:, 0])\n",
    "        evolutionary =          features['evolutionary']\n",
    "        secondary = tf.to_int32(features['secondary'][:, 0])\n",
    "        tertiary =              features['tertiary']\n",
    "        mask =                  features['mask'][:, 0]\n",
    "\n",
    "        pri_length = tf.size(primary)\n",
    "        keep = pri_length <= max_length\n",
    "\n",
    "        one_hot_primary = tf.one_hot(primary, NUM_AAS)\n",
    "\n",
    "        # Generate tertiary masking matrix--if mask is missing then assume all residues are present\n",
    "        mask = tf.cond(tf.not_equal(tf.size(mask), 0), lambda: mask, lambda: tf.ones([pri_length]))\n",
    "        ter_mask = masking_matrix(mask, name='ter_mask')        \n",
    "\n",
    "        return id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dihedral(p):\n",
    "    \"\"\"Praxeolitic formula\n",
    "    1 sqrt, 1 cross product\"\"\"\n",
    "    p0 = p[0]\n",
    "    p1 = p[1]\n",
    "    p2 = p[2]\n",
    "    p3 = p[3]\n",
    "\n",
    "    b0 = -1.0*(p1 - p0)\n",
    "    b1 = p2 - p1\n",
    "    b2 = p3 - p2\n",
    "\n",
    "    # normalize b1 so that it does not influence magnitude of vector\n",
    "    # rejections that come next\n",
    "    b1 /= np.linalg.norm(b1)\n",
    "\n",
    "    # vector rejections\n",
    "    # v = projection of b0 onto plane perpendicular to b1\n",
    "    #   = b0 minus component that aligns with b1\n",
    "    # w = projection of b2 onto plane perpendicular to b1\n",
    "    #   = b2 minus component that aligns with b1\n",
    "    v = b0 - np.dot(b0, b1)*b1\n",
    "    w = b2 - np.dot(b2, b1)*b1\n",
    "\n",
    "    # angle between v and w in a plane is the torsion angle\n",
    "    # v and w may not be normalized but that's fine since tan is y/x\n",
    "    x = np.dot(v, w)\n",
    "    y = np.dot(np.cross(b1, v), w)\n",
    "    return np.degrees(np.arctan2(y, x))\n",
    "\n",
    "def tf_rad2deg(rad):\n",
    "    pi_on_180 = 0.017453292519943295\n",
    "    return rad / pi_on_180\n",
    "\n",
    "# takes 1 dimensional tensor and outputs an angle\n",
    "def dihedral_tf1(p):\n",
    "    p0 = tf.gather(p, 0)\n",
    "    p1 = tf.gather(p, 1)\n",
    "    p2 = tf.gather(p, 2)\n",
    "    p3 = tf.gather(p, 3)\n",
    "    \n",
    "    b0 = -1.0 * (tf.subtract(p1, p0))\n",
    "    b1 = tf.subtract(p2, p1)\n",
    "    b2 = tf.subtract(p3, p2)\n",
    "    \n",
    "    b1 = tf.divide(b1, tf.norm(b1))\n",
    "    \n",
    "    v = tf.subtract(b0, tf.multiply(tf.tensordot(b0, b1, 1), b1))\n",
    "    w = tf.subtract(b2, tf.multiply(tf.tensordot(b2, b1, 1), b1))\n",
    "    \n",
    "    x = tf.tensordot(v, w, 1)\n",
    "    y = tf.tensordot(tf.cross(b1, v), w, 1)\n",
    "    \n",
    "    return tf_rad2deg(tf.atan2(y,x))\n",
    "\n",
    "# takes 2 dimensional tensor (K, 4) and outputs K angles\n",
    "def dihedral_tf2(p):\n",
    "    p0 = tf.gather(p, 0, axis=1)\n",
    "    p1 = tf.gather(p, 1, axis=1)\n",
    "    p2 = tf.gather(p, 2, axis=1)\n",
    "    p3 = tf.gather(p, 3, axis=1)\n",
    "    \n",
    "    b0 = -1.0 * (tf.subtract(p1, p0))\n",
    "    b1 = tf.subtract(p2, p1)\n",
    "    b2 = tf.subtract(p3, p2)\n",
    "    \n",
    "    b1 = tf.divide(b1, tf.norm(b1, axis=0))\n",
    "    \n",
    "    v = tf.subtract(b0, tf.multiply(tf.tensordot(b0, b1, 2), b1))\n",
    "    w = tf.subtract(b2, tf.multiply(tf.tensordot(b2, b1, 2), b1))\n",
    "    \n",
    "    x = tf.reduce_sum( tf.multiply( v, w ), 1, keepdims=True )\n",
    "    y = tf.reduce_sum( tf.multiply( tf.cross(b1, v), w ), 1, keepdims=True )\n",
    "\n",
    "    return tf_rad2deg(tf.atan2(y,x))\n",
    "\n",
    "# takes a 3 dimensional tensor (N, K, 4) and outputs (N,K) angles\n",
    "def dihedral_tf3(p):\n",
    "    p0 = tf.gather(p, 0, axis=2)\n",
    "    p1 = tf.gather(p, 1, axis=2)\n",
    "    p2 = tf.gather(p, 2, axis=2)\n",
    "    p3 = tf.gather(p, 3, axis=2)\n",
    "    \n",
    "    b0 = -1.0 * (tf.subtract(p1, p0))\n",
    "    b1 = tf.subtract(p2, p1)\n",
    "    b2 = tf.subtract(p3, p2)\n",
    "    \n",
    "    b1 = tf.divide(b1, tf.expand_dims(tf.norm(b1, axis=1), axis=1))\n",
    "    \n",
    "    v = tf.subtract(b0, tf.einsum('b,bij->bij', tf.einsum('bij,bij->b', b0, b1), b1))\n",
    "    w = tf.subtract(b2, tf.einsum('b,bij->bij', tf.einsum('bij,bij->b', b2, b1), b1))\n",
    "    \n",
    "    x = tf.reduce_sum( tf.multiply( v, w ), 2, keepdims=True )\n",
    "    y = tf.reduce_sum( tf.multiply( tf.cross(b1, v), w ), 2, keepdims=True )\n",
    "\n",
    "    return tf_rad2deg(tf.atan2(y,x))\n",
    "\n",
    "def slice_tf(tensor):\n",
    "    return tf.slice(tensor, (0,0), (4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need 4 coordinates to calculate an angle but the proteins are organized in 3x3 matricies. Thus how do we calculate an angle. Take 3 angles from 1 aminoacid and 1 angle from the next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = np.array([[\n",
    "#                 [ 1,           0,         0     ],\n",
    "#                 [ 0,           0,         0     ],\n",
    "#                 [ 0,           0,         1     ],\n",
    "#                 [ 0.999999,    0.000001,  1     ],\n",
    "#                 [ 0.999999,    0.000001,  1     ],\n",
    "#                 [ 0.999999,    0.000001,  1     ],\n",
    "#                 [ 0.999999,    0.000001,  1     ]\n",
    "#             ],[\n",
    "#                 [ 1,           0,         0     ],\n",
    "#                 [ 0,           0,         0     ],\n",
    "#                 [ 0,           0,         1     ],\n",
    "#                 [ 0.999999,    0.000001,  1     ],\n",
    "#                 [ 0.999999,    0.000001,  1     ],\n",
    "#                 [ 0.999999,    0.000001,  1     ],\n",
    "#                 [ 0.999999,    0.000001,  1     ]\n",
    "#             ]])\n",
    "# print(\"p1shape\", p1[0].shape)\n",
    "# p1_tf = tf.convert_to_tensor(p1[0])\n",
    "# test1 = test[0]\n",
    "# test_full\n",
    "\n",
    "# r = test1.shape[0]\n",
    "# n = 4\n",
    "# a_list = list(range(r))\n",
    "# the_list1 = np.array([a_list[slice(i, i+n)] for i in range(r - n+1)])\n",
    "# print(the_list1.shape, test1.shape)\n",
    "\n",
    "# r = test_full.shape[1]\n",
    "# n = 4\n",
    "# a_list = list(range(r))\n",
    "# the_list2 = [a_list[slice(i, i+n)] for i in range(r - n+1)]\n",
    "# the_list2 = np.array([the_list2 for _ in range(test_full.shape[0])])\n",
    "# the_list2 = the_list2\n",
    "# # print(the_list2.shape, test_full.shape)\n",
    "# # for i in range(len(r) - n + 1):\n",
    "# #     r[i: i + n]\n",
    "\n",
    "# p1_tf_stacked_full = tf.stack(tf.gather(test_full, the_list1, axis=1))\n",
    "\n",
    "# angle1 = dihedral_tf1(p1_tf)\n",
    "# angle2 = dihedral_tf2(p1_tf_stacked)\n",
    "# angle3 = dihedral_tf3(p1_tf_stacked_full)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     p1_tf_stacked_, p1_tf_stacked_full_, angle1_, angle2_, angle3_ = sess.run([p1_tf_stacked, p1_tf_stacked_full, angle1, angle2, angle3])\n",
    "\n",
    "# dihedral(p1[0]), np.array(p1_tf_stacked_).shape, np.array(p1_tf_stacked_full_).shape, angle1_, angle2_.shape, angle3_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array = np.array([[0, 1],[1, 2],[2, 3],[3, 4],[4, 5],[5, 6]])\n",
    "\n",
    "# r = array.shape[0]\n",
    "# n = 4\n",
    "# a_list = list(range(r))\n",
    "# the_list = np.array([a_list[slice(i, i+n)] for i in range(r - n+1)])\n",
    "\n",
    "# array[the_list].shape\n",
    "\n",
    "# print(array[the_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# the input pipeline should be rewritten using\n",
    "# the new dataset api that tesnorflow introduced\n",
    "\n",
    "# parameters for the training and\n",
    "# queues that control data flow from files\n",
    "num_epochs = 100\n",
    "batch_size=32\n",
    "capacity=1000\n",
    "min_after_dequeue=100\n",
    "lstm_units = 32\n",
    "\n",
    "# choose a path from which to get training data\n",
    "a_path = r'C:\\Users\\Michal\\Desktop\\ITU NLP\\casp7\\training\\30\\*'\n",
    "# load all the files from that path\n",
    "base_names = glob.glob(a_path)\n",
    "# convert names to a tensor and optionally restrict\n",
    "# how many files you want to use\n",
    "n_files = 2\n",
    "base_tensor = tf.convert_to_tensor(base_names[:n_files-1])\n",
    "\n",
    "# this queue is taking all the files and asynchronously\n",
    "# passes them forward (so that the rest of the computational\n",
    "# graph that actually does computation doesn't have to wait for new input)\n",
    "file_queue = tf.train.string_input_producer(\n",
    "    base_tensor,\n",
    "    num_epochs=num_epochs,\n",
    "    shuffle=True # not sure if this shuffle works\n",
    ")\n",
    "\n",
    "# the parsing that Al Quraishi provides to load the ProteinNet data\n",
    "res = read_protein(file_queue, max_length=1000)\n",
    "# unpacking the result\n",
    "id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep = res\n",
    "\n",
    "## I couldn't make shuffle batch work\n",
    "## because it doesn't have the dynamic padding included\n",
    "## workaround: https://github.com/tensorflow/tensorflow/issues/5147#issuecomment-271086206\n",
    "# ids, data, length = tf.train.shuffle_batch(\n",
    "#       [id_, one_hot_primary, pri_length], \n",
    "#       batch_size=batch_size, \n",
    "#       capacity=capacity,\n",
    "#       min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "# dynamic pad makes sure that the length of the proteins\n",
    "# is padded to the longest protein in the batch\n",
    "ids, data, labels, length = tf.train.batch(\n",
    "      [id_, one_hot_primary, tertiary, pri_length], \n",
    "      batch_size=batch_size, \n",
    "      capacity=capacity, \n",
    "      dynamic_pad=True\n",
    "    )\n",
    "\n",
    "# need the lengths to calculate the torsional angles\n",
    "protein_length = tf.gather(tf.shape(data), 1)\n",
    "protein_euc_length = tf.gather(tf.shape(labels), 1)\n",
    "\n",
    "###########\n",
    "# this part is calculating torsional angles\n",
    "# from euclidean coordinates\n",
    "# the reason for the placeholder is that I use\n",
    "# partial_run in the training loop, because I need to use\n",
    "# normal python in between tensorflow operations to calculate\n",
    "# this list\n",
    "the_list = tf.placeholder(tf.int32, shape=(None, 4))\n",
    "\n",
    "# selects all the possible slices of length 4\n",
    "p1_tf_stacked_full = tf.stack(tf.gather(labels, the_list, axis=1))\n",
    "# calculates torsional angles on the entire batch\n",
    "angle3 = dihedral_tf3(p1_tf_stacked_full)\n",
    "\n",
    "# adds 3 zeros at the end because I can't calculate the angle of\n",
    "# the last 3 atmos (need at least 4 atoms to calculate an angle)\n",
    "padding = tf.constant([[0, 0], [0,3], [0,0]])\n",
    "angle3 = tf.pad(angle3, padding)\n",
    "\n",
    "# reshaping the angles (because input is 3 times the length of normal protein)\n",
    "angle3_shape = tf.gather(tf.shape(angle3), [0,1])\n",
    "angle3 = tf.reshape(angle3, shape=angle3_shape)\n",
    "angle3 = tf.reshape(angle3, shape=(tf.gather(angle3_shape, 0), protein_length, 3))\n",
    "############\n",
    "\n",
    "# setting up a bidirectional LSTM\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=cell,\n",
    "    cell_bw=cell,\n",
    "    dtype=tf.float32,\n",
    "    inputs=data)\n",
    "# combining state from both directions\n",
    "outputs_conc = tf.concat(outputs, 2)\n",
    "# squeezing the output into tanh with 3 outputs\n",
    "pred = tf.layers.dense(outputs_conc, 3, activation=tf.nn.tanh, use_bias=False)\n",
    "# rescaling the output to match the scale of the angles (-180, 180)\n",
    "pred = tf.multiply(pred, tf.constant(180.))\n",
    "\n",
    "# choose a loss\n",
    "loss = tf.losses.absolute_difference(labels=angle3, predictions=pred)\n",
    "\n",
    "# learning rate placeholder for adaptive learning rate\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "# choose an optimizer to minimize the loss\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 73.5327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 50.76872\n",
      "Avg loss: 45.98293\n",
      "Avg loss: 43.35126\n",
      "Avg loss: 43.37757\n",
      "Avg loss: 41.057945\n",
      "Avg loss: 38.446465\n",
      "Avg loss: 38.186104\n",
      "Avg loss: 38.962242\n",
      "Avg loss: 36.217644\n",
      "Avg loss: 37.10598\n",
      "Avg loss: 38.297405\n",
      "Avg loss: 39.245018\n",
      "Avg loss: 36.2954\n",
      "Avg loss: 36.082634\n",
      "Avg loss: 35.567852\n",
      "Avg loss: 36.972862\n",
      "Avg loss: 35.645714\n",
      "Avg loss: 36.092323\n",
      "Avg loss: 35.988655\n",
      "Avg loss: 36.95144\n",
      "Avg loss: 35.402702\n",
      "Avg loss: 35.793438\n",
      "New learning rate: 0.02\n",
      "Avg loss: 34.426495\n",
      "Avg loss: 35.292675\n",
      "Avg loss: 33.65493\n",
      "Avg loss: 33.949295\n",
      "Avg loss: 33.798775\n",
      "Avg loss: 35.25446\n",
      "Avg loss: 33.653034\n",
      "Avg loss: 33.902493\n",
      "Avg loss: 33.609245\n",
      "Avg loss: 34.8739\n",
      "Avg loss: 33.350048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-5896f56bd999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m                                                                          pred, loss, dummy], \n\u001b[0;32m     49\u001b[0m                                                                          feed_dict={the_list: the_list_,\n\u001b[1;32m---> 50\u001b[1;33m                                                                                    learning_rate: feed_learning_rate})\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mpartial_run\u001b[1;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \"\"\"\n\u001b[0;32m    960\u001b[0m     \u001b[1;31m# TODO(touts): Support feeding and fetching the same tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpartial_run_setup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1321\u001b[0m                            run_metadata)\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_prun_fn\u001b[1;34m(handle, feed_dict, fetch_list)\u001b[0m\n\u001b[0;32m   1315\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'partial_run() requires empty target_list.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionprun\u001b[1;34m(self, handle, feed_dict, fetch_list)\u001b[0m\n\u001b[0;32m   1427\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m         return tf_session.TF_PRun(\n\u001b[1;32m-> 1429\u001b[1;33m             self._session, handle, feed_dict, fetch_list, status)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the main training loop. The tricky parts are the coord calls and using partial_run instead of normal run.\n",
    "\n",
    "The coord calls and try, exceptm, finally instructions are due to the way Queues operate.\n",
    "They use a coordinator and queue_runners to load the data asynchronously to the actual computations.\n",
    "\n",
    "partial_run is used because we need to use list comprehension to calculate the_list_.\n",
    "So we need to first get the sequence length of the given batch, use some normal python\n",
    "and then feed it back intro the graph and calculate the rest. If we called run twice,\n",
    "it would refetch a new batch on each call. partial_run is going to be unnecessary if\n",
    "we find a way of slicing out all slices of length 4 from a tensor in tensorflow (then\n",
    "it can be a part of the graph): \n",
    "https://stackoverflow.com/questions/53023727/how-to-efficiently-extract-all-slices-of-given-length-using-tensorflow.\n",
    "\"\"\"\n",
    "\n",
    "num_examples = 0\n",
    "init_learning_rate = 0.1\n",
    "\n",
    "# epsilon controls when to adapt learning rate\n",
    "epsilon = 0.1\n",
    "# controls when to stop training\n",
    "minimal_learning_rate = 1e-6\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # important to call both of these, because \n",
    "    # otherwise can't specify num_epochs in string_input_producer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "    try:\n",
    "        step = 0\n",
    "        \n",
    "        losses = []\n",
    "        avg_losses = []\n",
    "        \n",
    "        feed_learning_rate = init_learning_rate\n",
    "        while not coord.should_stop():\n",
    "            \n",
    "            with tf.control_dependencies([optimizer]):\n",
    "                dummy = tf.constant(0)\n",
    "                \n",
    "                h = sess.partial_run_setup([protein_euc_length, protein_length, angle3, \n",
    "                                            p1_tf_stacked_full, labels, pred,\n",
    "                                            optimizer, loss, dummy], [the_list, learning_rate])\n",
    "\n",
    "                protein_euc_length_, protein_length_ = sess.partial_run(h, [protein_euc_length, protein_length])\n",
    "\n",
    "                n = 4\n",
    "                a_list = list(range(protein_euc_length_))\n",
    "                the_list_ = np.array([a_list[slice(i, i+n)] for i in range(protein_euc_length_ - n+1)])            \n",
    "                \n",
    "                \n",
    "                angle3_, p1_tf_stacked_full_, labels_, pred_, loss_, _ = sess.partial_run(h, [angle3, \n",
    "                                                                         p1_tf_stacked_full, labels,\n",
    "                                                                         pred, loss, dummy], \n",
    "                                                                         feed_dict={the_list: the_list_,\n",
    "                                                                                   learning_rate: feed_learning_rate})\n",
    "\n",
    "                losses.append(loss_)\n",
    "                step += 1\n",
    "                if step % 10 == 0:\n",
    "                    avg_loss =  np.mean(losses)\n",
    "                    avg_losses.append(avg_loss)\n",
    "                    print(\"Avg loss:\", avg_loss)\n",
    "                    \n",
    "                    # adapt learning rate if the loss is not changing much\n",
    "                    if np.mean(avg_losses[-10:-7]) <= np.mean(avg_losses[-4:-1]) + epsilon:\n",
    "                        feed_learning_rate = feed_learning_rate / 5.\n",
    "                        print(\"New learning rate:\", feed_learning_rate)\n",
    "                        if feed_learning_rate < minimal_learning_rate:\n",
    "                            print(\"Learning rate too low. Stopping.\")\n",
    "                            coord.request_stop()\n",
    "                        \n",
    "                    losses = []\n",
    "\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "#         print(num_examples)\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-4c7e133a6c87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(avg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_examples = 0\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(tf.local_variables_initializer())\n",
    "#     coord = tf.train.Coordinator()  \n",
    "#     threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "# #     batch = tf.train.shuffle_batch([id_], 10, 200, 100)\n",
    "#     try:\n",
    "#         step = 0\n",
    "#         while not coord.should_stop():\n",
    "#             start_time = time.time()\n",
    "# #             ids_, data_, labels_, length_ = sess.run([ids, data, labels, length])\n",
    "            \n",
    "# #             out = sess.run(outputs)\n",
    "#             test, length_, outputs_, outputs_conc_, pred_ = sess.run([tile_test, length, outputs, outputs_conc, pred])\n",
    "# #             print(outputs_[0].shape, outputs_[1].shape, outputs_conc_.shape, pred_.shape, loss)\n",
    "            \n",
    "#             labels_ = sess.run([labels])\n",
    "            \n",
    "#             print(np.array(labels_).shape, np.array(labels_)[0][:2][:9])\n",
    "#             test = np.array(labels_)[0][:2]\n",
    "# #             print(ids_.shape, data_.shape, length_)\n",
    "# #             print \"grabbing\"\n",
    "# #             e, l = sess.run([example_batch, label_batch])\n",
    "# #             num_examples = num_examples + ids_.shape[0]\n",
    "# #             print \"num_examples = \" + str(num_examples)\n",
    "#             duration = time.time() - start_time\n",
    "\n",
    "#     except tf.errors.OutOfRangeError:\n",
    "#         print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "# #         print(num_examples)\n",
    "#     finally:\n",
    "#         # When done, ask the threads to stop.\n",
    "#         coord.request_stop()\n",
    "\n",
    "#         # Wait for threads to finish.\n",
    "#         coord.join(threads)\n",
    "#         sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
