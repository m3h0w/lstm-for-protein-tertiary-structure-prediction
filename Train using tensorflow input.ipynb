{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing provided by AlQuraishi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_AAS = 20\n",
    "NUM_DIMENSIONS = 3\n",
    "\n",
    "def masking_matrix(mask, name=None):\n",
    "    \"\"\" Constructs a masking matrix to zero out pairwise distances due to missing residues or padding. \n",
    "\n",
    "    Args:\n",
    "        mask: 0/1 vector indicating whether a position should be masked (0) or not (1)\n",
    "\n",
    "    Returns:\n",
    "        A square matrix with all 1s except for rows and cols whose corresponding indices in mask are set to 0.\n",
    "        [MAX_SEQ_LENGTH, MAX_SEQ_LENGTH]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'masking_matrix', [mask]) as scope:\n",
    "        mask = tf.convert_to_tensor(mask, name='mask')\n",
    "\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        base = tf.ones([tf.size(mask), tf.size(mask)])\n",
    "        matrix_mask = base * mask * tf.transpose(mask)\n",
    "\n",
    "        return matrix_mask\n",
    "        \n",
    "def read_protein(filename_queue, max_length, num_evo_entries=21, name=None):\n",
    "    \"\"\" Reads and parses a ProteinNet TF Record. \n",
    "\n",
    "        Primary sequences are mapped onto 20-dimensional one-hot vectors.\n",
    "        Evolutionary sequences are mapped onto num_evo_entries-dimensional real-valued vectors.\n",
    "        Secondary structures are mapped onto ints indicating one of 8 class labels.\n",
    "        Tertiary coordinates are flattened so that there are 3 times as many coordinates as \n",
    "        residues.\n",
    "\n",
    "        Evolutionary, secondary, and tertiary entries are optional.\n",
    "\n",
    "    Args:\n",
    "        filename_queue: TF queue for reading files\n",
    "        max_length:     Maximum length of sequence (number of residues) [MAX_LENGTH]. Not a \n",
    "                        TF tensor and is thus a fixed value.\n",
    "\n",
    "    Returns:\n",
    "        id: string identifier of record\n",
    "        one_hot_primary: AA sequence as one-hot vectors\n",
    "        evolutionary: PSSM sequence as vectors\n",
    "        secondary: DSSP sequence as int class labels\n",
    "        tertiary: 3D coordinates of structure\n",
    "        matrix_mask: Masking matrix to zero out pairwise distances in the masked regions\n",
    "        pri_length: Length of amino acid sequence\n",
    "        keep: True if primary length is less than or equal to max_length\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'read_protein', []) as scope:\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "        context, features = tf.parse_single_sequence_example(serialized_example,\n",
    "                                context_features={'id': tf.FixedLenFeature((1,), tf.string)},\n",
    "                                sequence_features={\n",
    "                                    'primary':      tf.FixedLenSequenceFeature((1,),               tf.int64),\n",
    "                                    'evolutionary': tf.FixedLenSequenceFeature((num_evo_entries,), tf.float32, allow_missing=True),\n",
    "                                    'secondary':    tf.FixedLenSequenceFeature((1,),               tf.int64,   allow_missing=True),\n",
    "                                    'tertiary':     tf.FixedLenSequenceFeature((NUM_DIMENSIONS,),  tf.float32, allow_missing=True),\n",
    "                                    'mask':         tf.FixedLenSequenceFeature((1,),               tf.float32, allow_missing=True)})\n",
    "        id_ = context['id'][0]\n",
    "        primary =   tf.to_int32(features['primary'][:, 0])\n",
    "        evolutionary =          features['evolutionary']\n",
    "        secondary = tf.to_int32(features['secondary'][:, 0])\n",
    "        tertiary =              features['tertiary']\n",
    "        mask =                  features['mask'][:, 0]\n",
    "\n",
    "        pri_length = tf.size(primary)\n",
    "        keep = pri_length <= max_length\n",
    "\n",
    "        one_hot_primary = tf.one_hot(primary, NUM_AAS)\n",
    "\n",
    "        # Generate tertiary masking matrix--if mask is missing then assume all residues are present\n",
    "        mask = tf.cond(tf.not_equal(tf.size(mask), 0), lambda: mask, lambda: tf.ones([pri_length]))\n",
    "        ter_mask = masking_matrix(mask, name='ter_mask')        \n",
    "\n",
    "        return id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of how to calculate dihedral angles and set up a bidirecitonal lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_rad2deg(rad):\n",
    "    pi_on_180 = 0.017453292519943295\n",
    "    return rad / pi_on_180\n",
    "\n",
    "# takes a 4-dimensional tensor (N, K, 4, 3) and outputs (N, K, 3) angles\n",
    "def dihedral_tf3(p):\n",
    "    p0 = tf.gather(p, 0, axis=2)\n",
    "    p1 = tf.gather(p, 1, axis=2)\n",
    "    p2 = tf.gather(p, 2, axis=2)\n",
    "    p3 = tf.gather(p, 3, axis=2)\n",
    "    \n",
    "    b0 = -1.0 * (tf.subtract(p1, p0))\n",
    "    b1 = tf.subtract(p2, p1)\n",
    "    b2 = tf.subtract(p3, p2)\n",
    "    \n",
    "    b1 = tf.divide(b1, tf.norm(b1, axis=2, keepdims=True))\n",
    "    b1 = tf.where(tf.is_nan(b1), tf.ones_like(b1), b1) # what to do when norm is 0?\n",
    "    \n",
    "    v = tf.subtract(b0, tf.einsum('bi,bij->bij', tf.einsum('bij,bij->bi', b0, b1), b1))\n",
    "    w = tf.subtract(b2, tf.einsum('bi,bij->bij', tf.einsum('bij,bij->bi', b2, b1), b1))\n",
    "    \n",
    "    x = tf.reduce_sum( tf.multiply( v, w ), 2, keepdims=True )\n",
    "    y = tf.reduce_sum( tf.multiply( tf.cross(b1, v), w ), 2, keepdims=True )\n",
    "\n",
    "    return tf_rad2deg(tf.atan2(y,x))\n",
    "\n",
    "# euclidean_coordinates are of shape (batch_size, protein_length, 3)\n",
    "def dihedral_pipeline(euclidean_coordinates, batch_size, protein_length):\n",
    "    # chooses all possible slices of length 4\n",
    "    euclidean_coordinates = euclidean_coordinates[:,:,:,None]\n",
    "    all_4_len_slices_euc_coord = tf.extract_image_patches(euclidean_coordinates,\n",
    "      ksizes=[1, 4, 3, 1],\n",
    "      strides=[1, 1, 1, 1],\n",
    "      rates=[1, 1, 1, 1],\n",
    "      padding='VALID')\n",
    "    all_4_len_slices_euc_coord = tf.reshape(tf.squeeze(all_4_len_slices_euc_coord), [batch_size, -1, 4, 3])\n",
    "\n",
    "    # calculates torsional angles on the entire batch\n",
    "    dihedral_angles = dihedral_tf3(all_4_len_slices_euc_coord)\n",
    "\n",
    "    # adds 3 zeros at the end because I can't calculate the angle of\n",
    "    # the last 3 atmos (need at least 4 atoms to calculate an angle)\n",
    "    padding = tf.constant([[0, 0], [0,3], [0,0]])\n",
    "    dihedral_angles = tf.pad(dihedral_angles, padding)\n",
    "\n",
    "    # reshaping the angles (because input is 3 times the length of normal protein)\n",
    "    dihedral_angles_shape = tf.gather(tf.shape(dihedral_angles), [0,1])\n",
    "    dihedral_angles = tf.reshape(dihedral_angles, shape=dihedral_angles_shape)\n",
    "    return tf.reshape(dihedral_angles, shape=(tf.gather(dihedral_angles_shape, 0), protein_length, 3))\n",
    "    \n",
    "# helper for setting up the bidirectional, multilayer lstm\n",
    "def bidirectional_lstm(input_data, num_layers, rnn_size, keep_prob, lengths):\n",
    "    output = input_data\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer),reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
    "\n",
    "            outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                              cell_bw, \n",
    "                                                              output,\n",
    "                                                              dtype=tf.float32,\n",
    "                                                              sequence_length=lengths)\n",
    "            output = tf.concat(outputs,2)\n",
    "\n",
    "    return output\n",
    "\n",
    "# helper to count number of records in a TF record file\n",
    "def get_num_records(tf_record_file):\n",
    "    return len([x for x in tf.python_io.tf_record_iterator(tf_record_file)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and load training paths. Count the number of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\30\\\\*', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\50\\\\*', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\70\\\\*', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\90\\\\*', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\95\\\\*', 'C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\100\\\\*']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108670"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose paths from which to get training data\n",
    "percentages = [30, 50, 70, 90, 95, 100]\n",
    "main_path = \"C:\\\\Users\\\\Michal\\\\Desktop\\\\ITU NLP\\\\casp7\\\\training\\\\\"\n",
    "paths = [main_path + str(perc) + '\\\\*' for perc in percentages]\n",
    "print(paths)\n",
    "# load all the file names from these paths\n",
    "base_names = [glob.glob(a_path) for a_path in paths]\n",
    "base_names = list(np.concatenate(base_names))\n",
    "\n",
    "training_samples = np.sum([get_num_records(file) for file in base_names])\n",
    "training_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# the input pipeline should be rewritten using\n",
    "# the new dataset api that tesnorflow introduced\n",
    "\n",
    "# parameters for the training and\n",
    "# queues that control data flow from files\n",
    "num_epochs = 100\n",
    "batch_size=64\n",
    "capacity=1000\n",
    "min_after_dequeue=100\n",
    "lstm_units = 32\n",
    "lstm_layers = 2\n",
    "\n",
    "# this queue is taking all the files and asynchronously\n",
    "# passes them forward (so that the rest of the computational\n",
    "# graph that actually does computation doesn't have to wait for new input)\n",
    "file_queue = tf.train.string_input_producer(\n",
    "    tf.convert_to_tensor(base_names),\n",
    "    num_epochs=num_epochs,\n",
    "    shuffle=True # not sure if this shuffle works\n",
    ")\n",
    "\n",
    "# the parsing that Al Quraishi provides to load the ProteinNet data\n",
    "res = read_protein(file_queue, max_length=1000)\n",
    "# unpacking the result\n",
    "id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep = res\n",
    "\n",
    "## I couldn't make shuffle batch work\n",
    "## because it doesn't have the dynamic padding included\n",
    "## workaround: https://github.com/tensorflow/tensorflow/issues/5147#issuecomment-271086206\n",
    "# ids, data, length = tf.train.shuffle_batch(\n",
    "#       [id_, one_hot_primary, pri_length], \n",
    "#       batch_size=batch_size, \n",
    "#       capacity=capacity,\n",
    "#       min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "# dynamic pad makes sure that the length of the proteins\n",
    "# is padded to the longest protein in the batch\n",
    "ids, one_hot_primary, evolutionary, labels, labels_mask, length = tf.train.batch(\n",
    "      [id_, one_hot_primary, evolutionary, tertiary, ter_mask, pri_length], \n",
    "      batch_size=batch_size, \n",
    "      capacity=capacity, \n",
    "      dynamic_pad=True\n",
    "    )\n",
    "\n",
    "# need the lengths to calculate the torsional angles\n",
    "protein_length = tf.gather(tf.shape(one_hot_primary), 1)\n",
    "protein_euc_length = tf.gather(tf.shape(labels), 1)\n",
    "\n",
    "# conver euclidean coordinates to dihedral angles\n",
    "dihedral_angles = dihedral_pipeline(labels, batch_size, protein_length)\n",
    "\n",
    "# prepare input data and setup the LSTM\n",
    "input_data = tf.concat([one_hot_primary, evolutionary], axis=2)\n",
    "outputs_conc = bidirectional_lstm(input_data=input_data, num_layers=lstm_layers, \n",
    "                                  rnn_size = lstm_units, keep_prob=0.05, lengths=length)\n",
    "\n",
    "# squeezing the output into tanh with 3 outputs\n",
    "pred = tf.layers.dense(outputs_conc, 3, activation=tf.nn.tanh, use_bias=False)\n",
    "# rescaling the output to match the scale of the angles (-180, 180)\n",
    "pred = tf.multiply(pred, tf.constant(180.))\n",
    "\n",
    "# choose a loss (mse or mae)\n",
    "loss = tf.losses.mean_squared_error(labels=dihedral_angles, predictions=pred)\n",
    "\n",
    "# learning rate placeholder for adaptive learning rate\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "# choose an optimizer to minimize the loss\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss, global_step=global_step)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 4966.0537\n",
      "Avg loss: 4647.9126\n",
      "Avg loss: 4632.4995\n",
      "Avg loss: 4341.1636\n",
      "Avg loss: 4666.87\n",
      "Avg loss: 4555.248\n",
      "Avg loss: 4361.123\n",
      "Avg loss: 4446.0483\n",
      "Avg loss: 4409.9014\n",
      "Avg loss: 4651.6577\n",
      "Avg loss: 4706.97\n",
      "Avg loss: 4592.3486\n",
      "Avg loss: 4551.559\n",
      "Avg loss: 4554.831\n",
      "Avg loss: 4307.3354\n",
      "Avg loss: 4483.675\n",
      "Avg loss: 4346.4033\n",
      "Avg loss: 4221.149\n",
      "Avg loss: 4650.21\n",
      "Avg loss: 4361.1123\n",
      "Avg loss: 4410.5864\n",
      "Avg loss: 4557.186\n",
      "Avg loss: 4415.791\n",
      "Avg loss: 4489.21\n",
      "Avg loss: 4186.879\n",
      "Avg loss: 4591.3545\n",
      "Avg loss: 4544.074\n",
      "Avg loss: 4565.371\n",
      "Avg loss: 4704.216\n",
      "Avg loss: 4327.8936\n",
      "Avg loss: 4617.077\n",
      "Avg loss: 4518.354\n",
      "Avg loss: 4632.3223\n",
      "Avg loss: 4721.375\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4679.312\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4278.651\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4586.857\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4501.4614\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4572.9126\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4452.034\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4463.1567\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4576.878\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4500.2676\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4267.984\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4523.3687\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4486.4907\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4551.794\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4569.2266\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4387.038\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4541.0107\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4695.839\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "Avg loss: 4465.5317\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n",
      "EPOCH\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the main training loop.\n",
    "\n",
    "The coord calls and try, exceptm, finally instructions are due to the way Queues operate.\n",
    "They use a coordinator and queue_runners to load the data asynchronously to the actual computations.\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # important to call both of these, because \n",
    "    # otherwise can't specify num_epochs in string_input_producer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "    try:\n",
    "        # we can't access information from the queue\n",
    "        # to know when an epoch ends, so we define our\n",
    "        # own step counter and calculate an average loss every n steps\n",
    "        step = 0\n",
    "        \n",
    "        losses = []\n",
    "        avg_losses = []\n",
    "        \n",
    "        feed_learning_rate = init_learning_rate\n",
    "        while not coord.should_stop():        \n",
    "                \n",
    "            dihedral_angles_, labels_, pred_, loss_ = sess.run([dihedral_angles, labels, pred, loss], \n",
    "                                                               feed_dict={learning_rate: feed_learning_rate})\n",
    "\n",
    "            losses.append(loss_)\n",
    "            if step % 50 == 0:\n",
    "                avg_loss =  np.mean(losses)\n",
    "                avg_losses.append(avg_loss)\n",
    "                print(\"Avg loss:\", avg_loss)\n",
    "\n",
    "                losses = []\n",
    "                \n",
    "            if step * batch_size > training_samples:\n",
    "                print(\"EPOCH\")\n",
    "                step = 0\n",
    "            \n",
    "            step += 1\n",
    "\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the avg losses over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(avg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get just the dihedral angles to see if they resemble how a ramachadran plot should look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    \n",
    "    dihedral_angles_ = sess.run([dihedral_angles])\n",
    "    coord.request_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array(dihedral_angles_)[0,:,:,0]\n",
    "beta = np.array(dihedral_angles_)[0,:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(beta[3], alpha[3], s=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
