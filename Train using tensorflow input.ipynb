{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing provided by AlQuraishi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AAS = 20\n",
    "NUM_DIMENSIONS = 3\n",
    "\n",
    "def masking_matrix(mask, name=None):\n",
    "    \"\"\" Constructs a masking matrix to zero out pairwise distances due to missing residues or padding. \n",
    "\n",
    "    Args:\n",
    "        mask: 0/1 vector indicating whether a position should be masked (0) or not (1)\n",
    "\n",
    "    Returns:\n",
    "        A square matrix with all 1s except for rows and cols whose corresponding indices in mask are set to 0.\n",
    "        [MAX_SEQ_LENGTH, MAX_SEQ_LENGTH]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'masking_matrix', [mask]) as scope:\n",
    "        mask = tf.convert_to_tensor(mask, name='mask')\n",
    "\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        base = tf.ones([tf.size(mask), tf.size(mask)])\n",
    "        matrix_mask = base * mask * tf.transpose(mask)\n",
    "\n",
    "        return matrix_mask\n",
    "        \n",
    "def read_protein(filename_queue, max_length, num_evo_entries=21, name=None):\n",
    "    \"\"\" Reads and parses a ProteinNet TF Record. \n",
    "\n",
    "        Primary sequences are mapped onto 20-dimensional one-hot vectors.\n",
    "        Evolutionary sequences are mapped onto num_evo_entries-dimensional real-valued vectors.\n",
    "        Secondary structures are mapped onto ints indicating one of 8 class labels.\n",
    "        Tertiary coordinates are flattened so that there are 3 times as many coordinates as \n",
    "        residues.\n",
    "\n",
    "        Evolutionary, secondary, and tertiary entries are optional.\n",
    "\n",
    "    Args:\n",
    "        filename_queue: TF queue for reading files\n",
    "        max_length:     Maximum length of sequence (number of residues) [MAX_LENGTH]. Not a \n",
    "                        TF tensor and is thus a fixed value.\n",
    "\n",
    "    Returns:\n",
    "        id: string identifier of record\n",
    "        one_hot_primary: AA sequence as one-hot vectors\n",
    "        evolutionary: PSSM sequence as vectors\n",
    "        secondary: DSSP sequence as int class labels\n",
    "        tertiary: 3D coordinates of structure\n",
    "        matrix_mask: Masking matrix to zero out pairwise distances in the masked regions\n",
    "        pri_length: Length of amino acid sequence\n",
    "        keep: True if primary length is less than or equal to max_length\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(name, 'read_protein', []) as scope:\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "        context, features = tf.parse_single_sequence_example(serialized_example,\n",
    "                                context_features={'id': tf.FixedLenFeature((1,), tf.string)},\n",
    "                                sequence_features={\n",
    "                                    'primary':      tf.FixedLenSequenceFeature((1,),               tf.int64),\n",
    "                                    'evolutionary': tf.FixedLenSequenceFeature((num_evo_entries,), tf.float32, allow_missing=True),\n",
    "                                    'secondary':    tf.FixedLenSequenceFeature((1,),               tf.int64,   allow_missing=True),\n",
    "                                    'tertiary':     tf.FixedLenSequenceFeature((NUM_DIMENSIONS,),  tf.float32, allow_missing=True),\n",
    "                                    'mask':         tf.FixedLenSequenceFeature((1,),               tf.float32, allow_missing=True)})\n",
    "        id_ = context['id'][0]\n",
    "        primary =   tf.to_int32(features['primary'][:, 0])\n",
    "        evolutionary =          features['evolutionary']\n",
    "        secondary = tf.to_int32(features['secondary'][:, 0])\n",
    "        tertiary =              features['tertiary']\n",
    "        mask =                  features['mask'][:, 0]\n",
    "\n",
    "        pri_length = tf.size(primary)\n",
    "        \n",
    "        keep = tf.ones_like(pri_length)\n",
    "        if max_length:\n",
    "            keep = pri_length <= max_length\n",
    "\n",
    "        one_hot_primary = tf.one_hot(primary, NUM_AAS)\n",
    "\n",
    "        # Generate tertiary masking matrix--if mask is missing then assume all residues are present\n",
    "        mask = tf.cond(tf.not_equal(tf.size(mask), 0), lambda: mask, lambda: tf.ones([pri_length]))\n",
    "        ter_mask = masking_matrix(mask, name='ter_mask')        \n",
    "\n",
    "        return id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of how to calculate dihedral angles and set up a bidirecitonal lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_rad2deg(rad):\n",
    "    pi_on_180 = 0.017453292519943295\n",
    "    return rad / pi_on_180\n",
    "\n",
    "# takes a 4-dimensional tensor (N, K, 4, 3) and outputs (N, K, 3) angles\n",
    "def dihedral_tf3(p):\n",
    "    p0 = tf.gather(p, 0, axis=2)\n",
    "    p1 = tf.gather(p, 1, axis=2)\n",
    "    p2 = tf.gather(p, 2, axis=2)\n",
    "    p3 = tf.gather(p, 3, axis=2)\n",
    "    \n",
    "    b0 = -1.0 * (tf.subtract(p1, p0))\n",
    "    b1 = tf.subtract(p2, p1)\n",
    "    b2 = tf.subtract(p3, p2)\n",
    "    \n",
    "    b1 = tf.divide(b1, tf.add(tf.norm(b1, axis=2, keepdims=True), tf.constant(1e-10)))\n",
    "#     b1 = tf.where(tf.is_nan(b1), tf.ones_like(b1), b1) # what to do when norm is 0?\n",
    "    \n",
    "    v = tf.subtract(b0, tf.einsum('bi,bij->bij', tf.einsum('bij,bij->bi', b0, b1), b1))\n",
    "    w = tf.subtract(b2, tf.einsum('bi,bij->bij', tf.einsum('bij,bij->bi', b2, b1), b1))\n",
    "    \n",
    "    x = tf.reduce_sum( tf.multiply( v, w ), 2, keepdims=True )\n",
    "    y = tf.reduce_sum( tf.multiply( tf.cross(b1, v), w ), 2, keepdims=True )\n",
    "\n",
    "#     return tf_rad2deg(tf.atan2(y,x))\n",
    "    return tf.atan2(y,x)\n",
    "\n",
    "# euclidean_coordinates are of shape (batch_size, protein_length, 3)\n",
    "def dihedral_pipeline(euclidean_coordinates, batch_size, protein_length):\n",
    "    # chooses all possible slices of length 4\n",
    "    euclidean_coordinates = euclidean_coordinates[:,:,:,None]\n",
    "    all_4_len_slices_euc_coord = tf.extract_image_patches(euclidean_coordinates,\n",
    "      ksizes=[1, 4, 3, 1],\n",
    "      strides=[1, 1, 1, 1],\n",
    "      rates=[1, 1, 1, 1],\n",
    "      padding='VALID')\n",
    "    all_4_len_slices_euc_coord = tf.reshape(tf.squeeze(all_4_len_slices_euc_coord), [batch_size, -1, 4, 3])\n",
    "\n",
    "    # calculates torsional angles on the entire batch\n",
    "    dihedral_angles = dihedral_tf3(all_4_len_slices_euc_coord)\n",
    "\n",
    "    # adds 3 zeros at the end because I can't calculate the angle of\n",
    "    # the last 3 atmos (need at least 4 atoms to calculate an angle)\n",
    "    padding = tf.constant([[0, 0], [0,3], [0,0]])\n",
    "    dihedral_angles = tf.pad(dihedral_angles, padding)\n",
    "\n",
    "    # reshaping the angles (because input is 3 times the length of normal protein)\n",
    "    dihedral_angles_shape = tf.gather(tf.shape(dihedral_angles), [0,1])\n",
    "    dihedral_angles = tf.reshape(dihedral_angles, shape=dihedral_angles_shape)\n",
    "    return tf.reshape(dihedral_angles, shape=(tf.gather(dihedral_angles_shape, 0), protein_length, 3))\n",
    "    \n",
    "# helper for setting up the bidirectional, multilayer lstm\n",
    "def bidirectional_lstm(input_data, num_layers, rnn_size, keep_prob, lengths):\n",
    "    output = input_data\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer),reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size, state_is_tuple = True)\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size, state_is_tuple = True)\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
    "\n",
    "            outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                              cell_bw, \n",
    "                                                              output,\n",
    "                                                              dtype=tf.float32,\n",
    "                                                              sequence_length=lengths)\n",
    "            output = tf.concat(outputs,2)\n",
    "\n",
    "    return output\n",
    "\n",
    "# helper to count number of records in a TF record file\n",
    "def get_num_records(tf_record_file):\n",
    "    return len([x for x in tf.python_io.tf_record_iterator(tf_record_file)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and load training paths. Count the number of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../Data/ProteinNet/casp7/training/50/*']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13024"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose paths from which to get training data\n",
    "percentages = [30, 50, 70, 90]\n",
    "percentages = [50]\n",
    "main_path = \"../../Data/ProteinNet/casp7/training/\"\n",
    "paths = [main_path + str(perc) + '/*' for perc in percentages]\n",
    "print(paths)\n",
    "# load all the file names from these paths\n",
    "base_names = [glob.glob(a_path) for a_path in paths]\n",
    "base_names = list(np.concatenate(base_names))\n",
    "\n",
    "training_samples = np.sum([get_num_records(file) for file in base_names])\n",
    "training_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# the input pipeline should be rewritten using\n",
    "# the new dataset api that tesnorflow introduced\n",
    "\n",
    "# parameters for the training and\n",
    "# queues that control data flow from files\n",
    "num_epochs = 100\n",
    "batch_size=32\n",
    "capacity=1000\n",
    "min_after_dequeue=100\n",
    "lstm_units = 500\n",
    "lstm_layers = 2\n",
    "\n",
    "limit_protein_size = None\n",
    "\n",
    "# this queue is taking all the files and asynchronously\n",
    "# passes them forward (so that the rest of the computational\n",
    "# graph that actually does computation doesn't have to wait for new input)\n",
    "file_queue = tf.train.string_input_producer(\n",
    "    tf.convert_to_tensor(base_names),\n",
    "    num_epochs=num_epochs,\n",
    "    shuffle=True # not sure if this shuffle works\n",
    ")\n",
    "\n",
    "# the parsing that Al Quraishi provides to load the ProteinNet data\n",
    "res = read_protein(file_queue, max_length=limit_protein_size)\n",
    "# unpacking the result\n",
    "id_, one_hot_primary, evolutionary, secondary, tertiary, ter_mask, pri_length, keep = res\n",
    "\n",
    "## I couldn't make shuffle batch work\n",
    "## because it doesn't have the dynamic padding included\n",
    "## workaround: https://github.com/tensorflow/tensorflow/issues/5147#issuecomment-271086206\n",
    "# ids, data, length = tf.train.shuffle_batch(\n",
    "#       [id_, one_hot_primary, pri_length], \n",
    "#       batch_size=batch_size, \n",
    "#       capacity=capacity,\n",
    "#       min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "# dynamic pad makes sure that the length of the proteins\n",
    "# is padded to the longest protein in the batch\n",
    "ids, one_hot_primary, evolutionary, labels, labels_mask, length, keep = tf.train.batch(\n",
    "      [id_, one_hot_primary, evolutionary, tertiary, ter_mask, pri_length, keep], \n",
    "      batch_size=batch_size, \n",
    "      capacity=capacity, \n",
    "      dynamic_pad=True\n",
    "    )\n",
    "\n",
    "if limit_protein_size:\n",
    "    ids = tf.boolean_mask(ids, keep, axis=0)\n",
    "    one_hot_primary = tf.boolean_mask(one_hot_primary, keep, axis=0)\n",
    "    evolutionary = tf.boolean_mask(evolutionary, keep, axis=0)\n",
    "    labels = tf.boolean_mask(labels, keep, axis=0)\n",
    "    labels_mask = tf.boolean_mask(labels_mask, keep, axis=0)\n",
    "    length = tf.boolean_mask(length, keep, axis=0)\n",
    "\n",
    "# refresh the batch_size because the keep mask could have\n",
    "# made it smaller\n",
    "batch_size = tf.shape(one_hot_primary)[0]\n",
    "\n",
    "## bucketting by length might speed up the trianing \n",
    "## https://github.com/tensorflow/tensorflow/issues/2354\n",
    "# tf.contrib.training.bucket_by_sequence_length(\n",
    "#     input_length,\n",
    "#     tensors,\n",
    "#     batch_size,\n",
    "#     bucket_boundaries,\n",
    "#     num_threads=1,\n",
    "#     capacity=32,\n",
    "#     bucket_capacities=None,\n",
    "#     shapes=None,\n",
    "#     dynamic_pad=False,\n",
    "#     allow_smaller_final_batch=False,\n",
    "#     keep_input=True,\n",
    "#     shared_name=None,\n",
    "#     name=None\n",
    "# )\n",
    "\n",
    "# need the lengths to calculate the torsional angles\n",
    "protein_length = tf.gather(tf.shape(one_hot_primary), 1)\n",
    "protein_euc_length = tf.gather(tf.shape(labels), 1)\n",
    "\n",
    "# conver euclidean coordinates to dihedral angles\n",
    "dihedral_angles = dihedral_pipeline(labels, batch_size, protein_length)\n",
    "dih_x = tf.cos(dihedral_angles)\n",
    "dih_y = tf.sin(dihedral_angles)\n",
    "dihedral_angles_deg = tf_rad2deg(dihedral_angles)\n",
    "tf.summary.histogram(\"dihedral_angles\", dihedral_angles)\n",
    "\n",
    "# prepare input data and setup the LSTM\n",
    "input_data = tf.concat([one_hot_primary, evolutionary], axis=2)\n",
    "# outputs_conc = bidirectional_lstm(input_data=input_data, num_layers=lstm_layers, \n",
    "#                                   rnn_size = lstm_units, keep_prob=0.05, lengths=length)\n",
    "\n",
    "# cells_fw = []\n",
    "# cells_bw = []\n",
    "# for _ in range(lstm_layers):\n",
    "#     cells_fw.append(tf.contrib.rnn.LSTMCell(lstm_units, state_is_tuple = True))\n",
    "#     cells_bw.append(tf.contrib.rnn.LSTMCell(lstm_units, state_is_tuple = True))\n",
    "    \n",
    "# outputs_conc, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "#     cells_fw=cells_fw,\n",
    "#     cells_bw=cells_bw,\n",
    "#     inputs = input_data,\n",
    "#     initial_states_fw=None,\n",
    "#     initial_states_bw=None,\n",
    "#     dtype=tf.float32,\n",
    "#     sequence_length=length,\n",
    "#     scope=None\n",
    "# )\n",
    "gru = tf.contrib.cudnn_rnn.CudnnGRU(\n",
    "    1, lstm_units,\n",
    "    kernel_initializer=tf.orthogonal_initializer())\n",
    "outputs_conc, _ = gru(tf.transpose(input_data, (1, 0, 2)))\n",
    "outputs_conc = tf.where(tf.is_nan(outputs_conc), tf.add(tf.zeros_like(outputs_conc), tf.constant(0.)), outputs_conc)\n",
    "tf.summary.histogram(\"input_data\", input_data)\n",
    "tf.summary.histogram(\"outputs_conc\", outputs_conc)\n",
    "\n",
    "# # squeezing the output into tanh with 3 outputs\n",
    "# pred_angles = tf.layers.dense(outputs_conc, 3, activation=tf.nn.tanh, use_bias=False)\n",
    "# # rescaling the output to match the scale of the angles (-180, 180)\n",
    "# pred_angles = tf.multiply(pred_angles, tf.constant(np.pi))\n",
    "# angle_loss = tf.losses.absolute_difference(labels=tf.squeeze(dihedral_angles), predictions=tf.squeeze(pred_angles))\n",
    "\n",
    "# squeezing the output into tanh with 6 outputs to represent the angles as\n",
    "# points on a plane\n",
    "# pred = tf.layers.dense(outputs_conc, 6, activation=tf.nn.tanh, use_bias=False, \n",
    "#                        kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.5))\n",
    "pred = tf.layers.dense(\n",
    "    tf.layers.batch_normalization(outputs_conc),\n",
    "    6, activation=tf.nn.tanh, \n",
    "    kernel_initializer=tf.orthogonal_initializer()\n",
    ")\n",
    "# pred = tf.subtract(pred, tf.constant(0.5))\n",
    "pred = tf.where(tf.is_nan(pred), tf.add(tf.zeros_like(pred), tf.constant(0.)), pred)\n",
    "tf.summary.histogram(\"pred\", pred)\n",
    "\n",
    "# x1, y1 = tf.gather(pred, 0, axis=2), tf.gather(pred, 1, axis=2)\n",
    "# x2, y2 = tf.gather(pred, 2, axis=2), tf.gather(pred, 3, axis=2)\n",
    "# x3, y3 = tf.gather(pred, 4, axis=2), tf.gather(pred, 5, axis=2)\n",
    "y1, y2, y3 = tf.gather(pred, 0, axis=2), tf.gather(pred, 1, axis=2), tf.gather(pred, 2, axis=2)\n",
    "x1, x2, x3 = tf.gather(pred, 3, axis=2), tf.gather(pred, 4, axis=2), tf.gather(pred, 5, axis=2)\n",
    "\n",
    "pred_angles = tf.concat([tf.expand_dims(tf.atan2(y1, x1), 2), \n",
    "                         tf.expand_dims(tf.atan2(y2, x2), 2), \n",
    "                         tf.expand_dims(tf.atan2(y3, x3), 2)], axis=2)\n",
    "pred_angles = tf.reshape(pred_angles, (batch_size, protein_length, 3))\n",
    "# pred_angles = tf.where(tf.is_nan(pred_angles), tf.zeros_like(pred_angles), pred_angles)\n",
    "tf.summary.histogram(\"pred_angles\", pred_angles)\n",
    "\n",
    "# choose a loss (mse or mae)\n",
    "loss = tf.losses.mean_squared_error(labels=tf.concat([dih_y, dih_x], axis=2), predictions=pred)\n",
    "# tf.summary.histogram(\"loss\", loss)\n",
    "\n",
    "angle_loss = tf.losses.mean_squared_error(labels=(dihedral_angles), predictions=pred_angles)\n",
    "tf.summary.histogram(\"angle_loss\", angle_loss)\n",
    "\n",
    "# learning rate placeholder for adaptive learning rate\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "# choose an optimizer to minimize the loss\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(angle_loss, global_step=global_step)\n",
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate).minimize(angle_loss)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "# Get the gradients\n",
    "gvs = optimizer.compute_gradients(angle_loss)\n",
    "# Clip gradients (except gradients from the dense layer)\n",
    "capped_gvs = [\n",
    "    (tf.clip_by_norm(grad, 2.), var) if not \n",
    "    var.name.startswith(\"dense\") else (grad, var)\n",
    "    for grad, var in gvs]\n",
    "# Apply Gradients (Update Trainable Variables) \n",
    "optimizer = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg angle loss: 6.1502013\n",
      "Std: 1.6034565\n",
      "Avg angle loss: 5.6030684\n",
      "Std: 0.02419396\n",
      "Avg angle loss: 5.616252\n",
      "Std: 0.031782616\n",
      "Avg angle loss: 5.6005697\n",
      "Std: 0.025229381\n",
      "Avg angle loss: 5.5919485\n",
      "Std: 0.027611416\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-9cc25a962805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m                                              \u001b[0mpred_angles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                              batch_size, pred], \n\u001b[0;32m---> 45\u001b[0;31m                                              feed_dict={learning_rate: 0.001})\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#             losses.append(loss_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the main training loop.\n",
    "\n",
    "The coord calls and try, exceptm, finally instructions are due to the way Queues operate.\n",
    "They use a coordinator and queue_runners to load the data asynchronously to the actual computations.\n",
    "\"\"\"\n",
    "logdir = \"./logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/train\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # important to call both of these, because \n",
    "    # otherwise can't specify num_epochs in string_input_producer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter( logdir, sess.graph)\n",
    "\n",
    "    try:\n",
    "        # we can't access information from the queue\n",
    "        # to know when an epoch ends, so we define our\n",
    "        # own step counter and calculate an average loss every n steps\n",
    "        step = 1\n",
    "        \n",
    "        losses = []\n",
    "        angle_losses = []\n",
    "        avg_losses = []\n",
    "        avg_angle_losses=[]\n",
    "        \n",
    "        samples_through_counter = 0\n",
    "        epochs_counter = 0\n",
    "        \n",
    "        while not coord.should_stop():  \n",
    "            merge = tf.summary.merge_all()\n",
    "#             (_, dihedral_angles_, labels_, pred_, \n",
    "#              pred_angles_, loss_, angle_loss_) = sess.run([optimizer, dihedral_angles, labels, pred, \n",
    "#                                                            pred_angles, loss, angle_loss], \n",
    "#                                                                feed_dict={learning_rate: 0.01})\n",
    "            (summary, _, dihedral_angles_, labels_, \n",
    "             pred_angles_, angle_loss_,\n",
    "             batch_size_, pred_) = sess.run([merge, optimizer, dihedral_angles, labels, \n",
    "                                             pred_angles, angle_loss,\n",
    "                                             batch_size, pred], \n",
    "                                             feed_dict={learning_rate: 0.001})\n",
    "\n",
    "#             losses.append(loss_)\n",
    "            angle_losses.append(angle_loss_)\n",
    "            if step % 10 == 0:\n",
    "#                 avg_loss =  np.mean(losses)\n",
    "                avg_angle_loss = np.mean(angle_losses)\n",
    "#                 avg_losses.append(avg_loss)\n",
    "                avg_angle_losses.append(avg_angle_loss)\n",
    "#                 print(\"Avg loss:\", avg_loss)\n",
    "                print(\"Avg angle loss:\", avg_angle_loss)\n",
    "#                 print(\"Angle loss:\", angle_loss_)\n",
    "                print(\"Std:\", np.std(angle_losses))\n",
    "\n",
    "                losses = []\n",
    "                angle_losses = []\n",
    "            \n",
    "            samples_through_counter += batch_size_\n",
    "            if samples_through_counter > training_samples * (epochs_counter+1):\n",
    "                print(\"EPOCH\")\n",
    "                epochs_counter += 1\n",
    "            \n",
    "            train_writer.add_summary(summary, step)\n",
    "            step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.14156, 3.1415927, -2.2531586, 3.1387393)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(dihedral_angles_), np.max(dihedral_angles_), np.min(pred_angles_), np.max(pred_angles_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the avg losses over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(avg_losses)\n",
    "plt.plot(avg_angle_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get just the dihedral angles to see if they resemble how a ramachadran plot should look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()  \n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    \n",
    "#     dihedral_angles_, euclidean_coordinates_, all_4_len_slices_euc_coord_ = sess.run([dihedral_angles, \n",
    "#                                                                                       euclidean_coordinates, \n",
    "#                                                                                       all_4_len_slices_euc_coord                                                                                     \n",
    "#                                                                                     ])\n",
    "    labels_mask_, x1_, x2_, x3_, y1_, y2_, y3_, pred_, pred_angles_, dihedral_angles_  = sess.run([labels_mask, x1, x2, x3, y1, y2, y3, pred, pred_angles, dihedral_angles])\n",
    "#     pred_ = sess.run([pred_c])\n",
    "    coord.request_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 484)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(atan1_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 789),\n",
       " (32, 789),\n",
       " (32, 789),\n",
       " (32, 789),\n",
       " (32, 789, 6),\n",
       " (32, 789, 3),\n",
       " (32, 789, 3),\n",
       " 9.444581)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_.shape, y2_.shape, y3_.shape, x3_.shape, pred_.shape, pred_angles_.shape, dihedral_angles_.shape, angle_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[   0. ],\n",
       "         [   0. ],\n",
       "         [   0. ]],\n",
       " \n",
       "        [[4515.9],\n",
       "         [4011.4],\n",
       "         [5703. ]],\n",
       " \n",
       "        [[4639.6],\n",
       "         [4043.6],\n",
       "         [5626.4]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[   0. ],\n",
       "         [   0. ],\n",
       "         [   0. ]],\n",
       " \n",
       "        [[   0. ],\n",
       "         [   0. ],\n",
       "         [   0. ]],\n",
       " \n",
       "        [[   0. ],\n",
       "         [   0. ],\n",
       "         [   0. ]]], dtype=float32), array([[[   0. ,    0. ,    0. ],\n",
       "         [4515.9, 4011.4, 5703. ],\n",
       "         [4639.6, 4043.6, 5626.4],\n",
       "         [4608.7, 4049.5, 5477.2]],\n",
       " \n",
       "        [[4515.9, 4011.4, 5703. ],\n",
       "         [4639.6, 4043.6, 5626.4],\n",
       "         [4608.7, 4049.5, 5477.2],\n",
       "         [4548. , 3942.8, 5425.9]],\n",
       " \n",
       "        [[4639.6, 4043.6, 5626.4],\n",
       "         [4608.7, 4049.5, 5477.2],\n",
       "         [4548. , 3942.8, 5425.9],\n",
       "         [4505. , 3936.9, 5286.2]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[   0. ,    0. ,    0. ],\n",
       "         [   0. ,    0. ,    0. ],\n",
       "         [   0. ,    0. ,    0. ],\n",
       "         [   0. ,    0. ,    0. ]],\n",
       " \n",
       "        [[   0. ,    0. ,    0. ],\n",
       "         [   0. ,    0. ,    0. ],\n",
       "         [   0. ,    0. ,    0. ],\n",
       "         [   0. ,    0. ,    0. ]],\n",
       " \n",
       "        [[   0. ,    0. ,    0. ],\n",
       "         [   0. ,    0. ,    0. ],\n",
       "         [   0. ,    0. ,    0. ],\n",
       "         [   0. ,    0. ,    0. ]]], dtype=float32), array([[-0.25348714,  3.0816522 , -1.538865  ],\n",
       "        [-0.08465115, -3.0919905 , -1.3822418 ],\n",
       "        [ 2.2193382 ,  3.0607564 , -1.8549333 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]], dtype=float32))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_coordinates_[0][2:], all_4_len_slices_euc_coord_[0][2:], dihedral_angles_[0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 637, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dihedral_angles_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = tf_rad2deg(np.array(dihedral_angles_)[:,:,0])\n",
    "beta = tf_rad2deg(np.array(dihedral_angles_)[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-180, 180)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEUHBQEREUDAQEREUDEREBAUDEREB/j8gYno7nwKrbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14dfab349b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(beta[9], alpha[9], s=2)\n",
    "plt.xlim((-180,180))\n",
    "plt.ylim((-180,180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
